{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jainriya9/DeepFake-Detection/blob/main/pipeline_formation%2C_still_overfit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uck-qcHNqukA",
        "outputId": "95dde5d6-02ba-47dc-a4f0-9fff11d2655f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbpBhP_3qydc",
        "outputId": "7d087d41-8640-42df-c98e-1b54a78c7c14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['List_of_testing_videos.txt', 'Celeb-synthesis', 'Celeb-real', 'YouTube-real', 'Frames-Real', 'Frames-Fake', 'Frames-Real-20240626T130257Z-001.zip', 'Frames-Fake-20240626T130258Z-001.zip', 'Frames-YouTube-real', 'model.h5', 'Split-Frames']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.listdir('/content/drive/MyDrive/Celeb-DF'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g34ya7LDqyaH",
        "outputId": "ae2b82c8-b414-4f33-8659-6fef0704363f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['id0_0000.mp4', 'id0_0001.mp4', 'id0_0002.mp4', 'id0_0005.mp4', 'id0_0006.mp4', 'id10_0000.mp4', 'id0_0009.mp4', 'id0_0003.mp4', 'id0_0008.mp4', 'id0_0007.mp4', 'id0_0004.mp4', 'id11_0005.mp4', 'id10_0001.mp4', 'id10_0006.mp4', 'id11_0006.mp4', 'id10_0002.mp4', 'id10_0008.mp4', 'id10_0007.mp4', 'id10_0003.mp4', 'id10_0009.mp4', 'id10_0005.mp4', 'id11_0003.mp4', 'id11_0002.mp4', 'id11_0000.mp4', 'id11_0001.mp4', 'id10_0004.mp4', 'id11_0004.mp4', 'id12_0004.mp4', 'id13_0005.mp4', 'id13_0008.mp4', 'id13_0000.mp4', 'id11_0008.mp4', 'id13_0007.mp4', 'id13_0009.mp4', 'id13_0006.mp4', 'id11_0010.mp4', 'id12_0000.mp4', 'id12_0001.mp4', 'id13_0004.mp4', 'id11_0009.mp4', 'id13_0001.mp4', 'id12_0003.mp4', 'id11_0007.mp4', 'id12_0006.mp4', 'id13_0003.mp4', 'id12_0005.mp4', 'id13_0002.mp4', 'id12_0002.mp4', 'id13_0015.mp4', 'id13_0014.mp4', 'id16_0003.mp4', 'id13_0013.mp4', 'id13_0010.mp4', 'id16_0000.mp4', 'id13_0012.mp4', 'id16_0001.mp4', 'id16_0002.mp4', 'id13_0011.mp4', 'id17_0006.mp4', 'id16_0005.mp4', 'id16_0008.mp4', 'id17_0007.mp4', 'id16_0009.mp4', 'id17_0005.mp4', 'id16_0004.mp4', 'id16_0012.mp4', 'id17_0003.mp4', 'id16_0010.mp4', 'id16_0013.mp4', 'id17_0004.mp4', 'id17_0000.mp4', 'id16_0011.mp4', 'id17_0001.mp4', 'id17_0002.mp4', 'id16_0006.mp4', 'id16_0007.mp4', 'id2_0005.mp4', 'id2_0008.mp4', 'id1_0001.mp4', 'id2_0007.mp4', 'id17_0009.mp4', 'id1_0000.mp4', 'id1_0005.mp4', 'id2_0002.mp4', 'id1_0006.mp4', 'id17_0008.mp4', 'id1_0002.mp4', 'id1_0007.mp4', 'id2_0003.mp4', 'id1_0009.mp4', 'id1_0008.mp4', 'id2_0006.mp4', 'id1_0003.mp4', 'id2_0000.mp4', 'id2_0004.mp4', 'id1_0004.mp4', 'id2_0001.mp4', 'id2_0009.mp4', 'id3_0009.mp4', 'id3_0001.mp4', 'id3_0003.mp4', 'id4_0003.mp4', 'id3_0005.mp4', 'id3_0000.mp4', 'id3_0004.mp4', 'id4_0001.mp4', 'id3_0006.mp4', 'id4_0002.mp4', 'id3_0007.mp4', 'id4_0000.mp4', 'id3_0002.mp4', 'id3_0008.mp4', 'id6_0002.mp4', 'id4_0009.mp4', 'id6_0003.mp4', 'id4_0006.mp4', 'id6_0001.mp4', 'id4_0007.mp4', 'id6_0000.mp4', 'id6_0005.mp4', 'id4_0004.mp4', 'id4_0005.mp4', 'id6_0004.mp4', 'id4_0008.mp4', 'id6_0006.mp4', 'id7_0007.mp4', 'id8_0002.mp4', 'id7_0005.mp4', 'id7_0004.mp4', 'id8_0003.mp4', 'id7_0009.mp4', 'id7_0001.mp4', 'id7_0000.mp4', 'id6_0007.mp4', 'id8_0004.mp4', 'id8_0000.mp4', 'id8_0005.mp4', 'id7_0002.mp4', 'id8_0001.mp4', 'id7_0006.mp4', 'id7_0003.mp4', 'id7_0008.mp4', 'id6_0008.mp4', 'id6_0009.mp4', 'id9_0001.mp4', 'id9_0002.mp4', 'id9_0000.mp4', 'id8_0006.mp4', 'id9_0006.mp4', 'id8_0007.mp4', 'id9_0005.mp4', 'id9_0004.mp4', 'id8_0008.mp4', 'id9_0003.mp4', 'id8_0009.mp4', 'id9_0009.mp4', 'id9_0008.mp4', 'id9_0007.mp4']\n",
            "['id0_id16_0008.mp4', 'id0_id16_0007.mp4', 'id0_id16_0004.mp4', 'id0_id16_0005.mp4', 'id0_id16_0006.mp4', 'id0_id16_0003.mp4', 'id0_id16_0000.mp4', 'id0_id16_0001.mp4', 'id0_id16_0002.mp4', 'id0_id1_0001.mp4', 'id0_id17_0006.mp4', 'id0_id17_0009.mp4', 'id0_id1_0005.mp4', 'id0_id17_0002.mp4', 'id0_id17_0001.mp4', 'id0_id1_0002.mp4', 'id0_id1_0006.mp4', 'id0_id17_0003.mp4', 'id0_id16_0009.mp4', 'id0_id1_0000.mp4', 'id0_id17_0005.mp4', 'id0_id17_0007.mp4', 'id0_id1_0003.mp4', 'id0_id1_0007.mp4', 'id0_id17_0000.mp4', 'id0_id2_0008.mp4', 'id0_id2_0003.mp4', 'id0_id2_0007.mp4', 'id0_id2_0009.mp4', 'id0_id3_0000.mp4', 'id0_id2_0004.mp4', 'id0_id2_0005.mp4', 'id0_id1_0009.mp4', 'id0_id2_0006.mp4', 'id0_id2_0001.mp4', 'id0_id2_0000.mp4', 'id0_id2_0002.mp4', 'id0_id3_0002.mp4', 'id0_id4_0000.mp4', 'id0_id3_0004.mp4', 'id0_id4_0001.mp4', 'id0_id3_0008.mp4', 'id0_id3_0005.mp4', 'id0_id3_0006.mp4', 'id0_id3_0001.mp4', 'id0_id3_0009.mp4', 'id0_id3_0003.mp4', 'id0_id3_0007.mp4', 'id0_id6_0006.mp4', 'id0_id4_0006.mp4', 'id0_id9_0005.mp4', 'id0_id4_0003.mp4', 'id0_id6_0000.mp4', 'id0_id9_0002.mp4', 'id0_id9_0001.mp4', 'id0_id6_0002.mp4', 'id0_id6_0001.mp4', 'id0_id6_0007.mp4', 'id0_id4_0005.mp4', 'id0_id9_0000.mp4', 'id0_id9_0003.mp4', 'id0_id9_0006.mp4', 'id0_id4_0009.mp4', 'id0_id4_0002.mp4', 'id0_id4_0004.mp4', 'id0_id6_0009.mp4', 'id10_id12_0003.mp4', 'id10_id11_0001.mp4', 'id10_id12_0002.mp4', 'id0_id9_0008.mp4', 'id10_id11_0008.mp4', 'id10_id11_0005.mp4', 'id0_id9_0007.mp4', 'id10_id11_0006.mp4', 'id10_id11_0007.mp4', 'id10_id11_0004.mp4', 'id0_id9_0009.mp4', 'id10_id11_0002.mp4', 'id10_id12_0001.mp4', 'id10_id11_0003.mp4', 'id10_id7_0001.mp4', 'id10_id12_0004.mp4', 'id10_id13_0007.mp4', 'id10_id7_0002.mp4', 'id10_id13_0002.mp4', 'id10_id13_0004.mp4', 'id10_id13_0005.mp4', 'id10_id12_0007.mp4', 'id10_id12_0006.mp4', 'id10_id13_0001.mp4', 'id10_id13_0008.mp4', 'id10_id12_0008.mp4', 'id10_id13_0003.mp4', 'id10_id12_0005.mp4', 'id10_id13_0006.mp4', 'id12_id10_0004.mp4', 'id12_id10_0002.mp4', 'id11_id7_0007.mp4', 'id10_id7_0009.mp4', 'id11_id7_0005.mp4', 'id11_id7_0002.mp4', 'id10_id7_0006.mp4', 'id11_id7_0000.mp4', 'id11_id7_0003.mp4', 'id11_id7_0004.mp4', 'id11_id7_0006.mp4', 'id10_id7_0004.mp4', 'id11_id7_0010.mp4', 'id10_id7_0005.mp4', 'id11_id7_0009.mp4', 'id10_id7_0008.mp4', 'id11_id7_0001.mp4', 'id10_id7_0007.mp4', 'id10_id7_0003.mp4', 'id11_id7_0008.mp4', 'id12_id10_0001.mp4', 'id12_id7_0006.mp4', 'id13_id10_0005.mp4', 'id13_id10_0001.mp4', 'id13_id10_0003.mp4', 'id12_id7_0004.mp4', 'id12_id7_0002.mp4', 'id13_id10_0011.mp4', 'id13_id10_0007.mp4', 'id12_id10_0005.mp4', 'id12_id7_0000.mp4', 'id12_id7_0005.mp4', 'id12_id10_0006.mp4', 'id13_id10_0000.mp4', 'id13_id10_0008.mp4', 'id13_id10_0004.mp4', 'id12_id7_0001.mp4', 'id13_id7_0003.mp4', 'id13_id7_0008.mp4', 'id13_id10_0012.mp4', 'id13_id7_0004.mp4', 'id13_id7_0005.mp4', 'id13_id7_0002.mp4', 'id13_id7_0000.mp4', 'id13_id7_0011.mp4', 'id13_id7_0007.mp4', 'id13_id7_0001.mp4', 'id13_id7_0009.mp4', 'id13_id10_0014.mp4', 'id13_id10_0015.mp4', 'id16_id0_0010.mp4', 'id16_id0_0000.mp4', 'id16_id17_0004.mp4', 'id16_id17_0000.mp4', 'id16_id17_0001.mp4', 'id13_id7_0015.mp4', 'id16_id0_0006.mp4', 'id16_id0_0013.mp4', 'id16_id0_0012.mp4', 'id16_id0_0003.mp4', 'id16_id17_0005.mp4', 'id16_id0_0005.mp4', 'id16_id0_0011.mp4', 'id16_id0_0002.mp4', 'id16_id0_0004.mp4', 'id16_id0_0001.mp4', 'id16_id17_0003.mp4', 'id16_id0_0007.mp4', 'id16_id17_0002.mp4', 'id13_id7_0012.mp4', 'id16_id0_0009.mp4', 'id16_id17_0011.mp4', 'id16_id1_0006.mp4', 'id16_id17_0010.mp4', 'id16_id1_0004.mp4', 'id16_id1_0010.mp4', 'id16_id1_0000.mp4', 'id16_id17_0013.mp4', 'id16_id1_0001.mp4', 'id16_id1_0007.mp4', 'id16_id1_0002.mp4', 'id16_id17_0009.mp4', 'id16_id1_0005.mp4', 'id16_id17_0012.mp4', 'id16_id1_0011.mp4', 'id16_id1_0003.mp4', 'id16_id17_0007.mp4', 'id16_id17_0006.mp4', 'id16_id1_0009.mp4', 'id16_id2_0001.mp4', 'id16_id2_0005.mp4', 'id16_id2_0000.mp4', 'id16_id1_0013.mp4', 'id16_id2_0007.mp4', 'id16_id2_0004.mp4', 'id16_id2_0003.mp4', 'id16_id2_0009.mp4', 'id16_id2_0010.mp4', 'id16_id2_0002.mp4', 'id16_id1_0012.mp4', 'id16_id2_0011.mp4', 'id16_id2_0006.mp4', 'id16_id6_0005.mp4', 'id16_id2_0012.mp4', 'id16_id3_0007.mp4', 'id16_id3_0011.mp4', 'id16_id3_0002.mp4', 'id16_id3_0006.mp4', 'id16_id6_0006.mp4', 'id16_id6_0002.mp4', 'id16_id3_0012.mp4', 'id16_id6_0003.mp4', 'id16_id2_0013.mp4', 'id16_id6_0001.mp4', 'id16_id3_0009.mp4', 'id16_id3_0004.mp4', 'id16_id6_0000.mp4', 'id16_id3_0000.mp4', 'id16_id3_0001.mp4', 'id16_id6_0004.mp4', 'id16_id3_0010.mp4', 'id16_id3_0003.mp4', 'id16_id3_0005.mp4', 'id16_id3_0013.mp4', 'id16_id9_0003.mp4', 'id16_id9_0009.mp4', 'id16_id6_0010.mp4', 'id16_id9_0001.mp4', 'id16_id6_0012.mp4', 'id16_id6_0011.mp4', 'id16_id6_0007.mp4', 'id16_id9_0000.mp4', 'id16_id6_0013.mp4', 'id16_id9_0011.mp4', 'id16_id9_0004.mp4', 'id16_id9_0007.mp4', 'id16_id9_0012.mp4', 'id16_id9_0010.mp4', 'id16_id9_0006.mp4', 'id16_id6_0009.mp4', 'id16_id9_0005.mp4', 'id16_id9_0002.mp4', 'id17_id16_0001.mp4', 'id17_id0_0001.mp4', 'id17_id0_0004.mp4', 'id16_id9_0013.mp4', 'id17_id16_0002.mp4', 'id17_id0_0003.mp4', 'id17_id0_0007.mp4', 'id17_id0_0000.mp4', 'id17_id0_0002.mp4', 'id17_id16_0000.mp4', 'id17_id0_0006.mp4', 'id17_id0_0009.mp4', 'id17_id0_0005.mp4', 'id17_id16_0003.mp4', 'id17_id1_0007.mp4', 'id17_id1_0000.mp4', 'id17_id16_0009.mp4', 'id17_id16_0007.mp4', 'id17_id2_0001.mp4', 'id17_id2_0004.mp4', 'id17_id2_0002.mp4', 'id17_id1_0001.mp4', 'id17_id16_0005.mp4', 'id17_id1_0002.mp4', 'id17_id1_0009.mp4', 'id17_id16_0006.mp4', 'id17_id2_0005.mp4', 'id17_id1_0004.mp4', 'id17_id16_0004.mp4', 'id17_id2_0000.mp4', 'id17_id2_0003.mp4', 'id17_id1_0003.mp4', 'id17_id1_0005.mp4', 'id17_id1_0006.mp4', 'id17_id3_0002.mp4', 'id17_id2_0009.mp4', 'id17_id3_0009.mp4', 'id17_id6_0002.mp4', 'id17_id3_0005.mp4', 'id17_id3_0006.mp4', 'id17_id3_0004.mp4', 'id17_id6_0006.mp4', 'id17_id6_0000.mp4', 'id17_id6_0003.mp4', 'id17_id2_0007.mp4', 'id17_id3_0001.mp4', 'id17_id3_0007.mp4', 'id17_id6_0001.mp4', 'id17_id2_0006.mp4', 'id17_id3_0000.mp4', 'id17_id6_0005.mp4', 'id17_id3_0003.mp4', 'id17_id6_0004.mp4', 'id17_id9_0003.mp4', 'id17_id9_0009.mp4', 'id1_id0_0000.mp4', 'id17_id9_0007.mp4', 'id17_id9_0006.mp4', 'id17_id9_0001.mp4', 'id17_id9_0002.mp4', 'id17_id6_0009.mp4', 'id1_id0_0001.mp4', 'id17_id6_0007.mp4', 'id17_id9_0005.mp4', 'id17_id9_0004.mp4', 'id17_id9_0000.mp4', 'id1_id0_0002.mp4', 'id1_id16_0004.mp4', 'id1_id0_0004.mp4', 'id1_id16_0005.mp4', 'id1_id16_0002.mp4', 'id1_id17_0003.mp4', 'id1_id17_0002.mp4', 'id1_id17_0000.mp4', 'id1_id0_0007.mp4', 'id1_id0_0009.mp4', 'id1_id17_0001.mp4', 'id1_id16_0006.mp4', 'id1_id16_0000.mp4', 'id1_id0_0005.mp4', 'id1_id16_0009.mp4', 'id1_id17_0005.mp4', 'id1_id0_0006.mp4', 'id1_id16_0007.mp4', 'id1_id16_0001.mp4', 'id1_id16_0003.mp4', 'id1_id17_0004.mp4', 'id1_id0_0003.mp4', 'id1_id3_0009.mp4', 'id1_id3_0003.mp4', 'id1_id2_0003.mp4', 'id1_id4_0000.mp4', 'id1_id2_0007.mp4', 'id1_id3_0000.mp4', 'id1_id2_0004.mp4', 'id1_id3_0005.mp4', 'id1_id17_0009.mp4', 'id1_id2_0006.mp4', 'id1_id2_0001.mp4', 'id1_id2_0009.mp4', 'id1_id2_0002.mp4', 'id1_id2_0000.mp4', 'id1_id3_0004.mp4', 'id1_id3_0001.mp4', 'id1_id17_0006.mp4', 'id1_id17_0007.mp4', 'id1_id3_0006.mp4', 'id1_id2_0005.mp4', 'id1_id3_0007.mp4', 'id1_id3_0002.mp4', 'id1_id4_0002.mp4', 'id1_id6_0005.mp4', 'id1_id4_0001.mp4', 'id1_id4_0006.mp4', 'id1_id4_0007.mp4', 'id1_id6_0000.mp4', 'id1_id6_0001.mp4', 'id1_id4_0009.mp4', 'id1_id4_0005.mp4', 'id1_id6_0002.mp4', 'id1_id6_0004.mp4', 'id1_id4_0003.mp4', 'id1_id6_0003.mp4', 'id1_id4_0004.mp4', 'id2_id0_0000.mp4', 'id1_id9_0003.mp4', 'id1_id9_0005.mp4', 'id1_id9_0000.mp4', 'id1_id9_0009.mp4', 'id1_id9_0007.mp4', 'id1_id6_0006.mp4', 'id1_id6_0009.mp4', 'id1_id9_0004.mp4', 'id1_id9_0006.mp4', 'id1_id9_0002.mp4', 'id2_id0_0001.mp4', 'id1_id9_0001.mp4', 'id1_id6_0007.mp4', 'id2_id0_0003.mp4', 'id2_id16_0006.mp4', 'id2_id16_0001.mp4', 'id2_id0_0009.mp4', 'id2_id0_0002.mp4', 'id2_id0_0004.mp4', 'id2_id16_0005.mp4', 'id2_id16_0004.mp4', 'id2_id0_0008.mp4', 'id2_id0_0007.mp4', 'id2_id16_0008.mp4', 'id2_id16_0002.mp4', 'id2_id0_0006.mp4', 'id2_id16_0003.mp4', 'id2_id17_0000.mp4', 'id2_id16_0009.mp4', 'id2_id16_0000.mp4', 'id2_id0_0005.mp4', 'id2_id16_0007.mp4', 'id2_id1_0001.mp4', 'id2_id1_0003.mp4', 'id2_id17_0001.mp4', 'id2_id1_0000.mp4', 'id2_id17_0002.mp4', 'id2_id17_0009.mp4', 'id2_id17_0003.mp4', 'id2_id17_0008.mp4', 'id2_id17_0005.mp4', 'id2_id1_0002.mp4', 'id2_id17_0006.mp4', 'id2_id17_0004.mp4', 'id2_id17_0007.mp4', 'id2_id3_0003.mp4', 'id2_id1_0007.mp4', 'id2_id3_0002.mp4', 'id2_id1_0005.mp4', 'id2_id3_0000.mp4', 'id2_id1_0008.mp4', 'id2_id3_0001.mp4', 'id2_id3_0006.mp4', 'id2_id3_0005.mp4', 'id2_id3_0004.mp4', 'id2_id3_0009.mp4', 'id2_id1_0004.mp4', 'id2_id1_0009.mp4', 'id2_id3_0008.mp4', 'id2_id3_0007.mp4', 'id2_id1_0006.mp4', 'id2_id4_0008.mp4', 'id2_id6_0005.mp4', 'id2_id6_0003.mp4', 'id2_id6_0004.mp4', 'id2_id6_0006.mp4', 'id2_id4_0006.mp4', 'id2_id4_0003.mp4', 'id2_id4_0002.mp4', 'id2_id6_0001.mp4', 'id2_id6_0007.mp4', 'id2_id4_0000.mp4', 'id2_id9_0000.mp4', 'id2_id4_0001.mp4', 'id2_id4_0009.mp4', 'id2_id6_0002.mp4', 'id2_id4_0007.mp4', 'id2_id6_0008.mp4', 'id2_id6_0009.mp4', 'id2_id9_0001.mp4', 'id2_id9_0003.mp4', 'id2_id9_0008.mp4', 'id3_id0_0001.mp4', 'id3_id0_0002.mp4', 'id2_id9_0006.mp4', 'id2_id9_0007.mp4', 'id2_id9_0002.mp4', 'id3_id0_0000.mp4', 'id2_id9_0009.mp4', 'id3_id0_0003.mp4', 'id2_id9_0004.mp4', 'id2_id9_0005.mp4', 'id3_id17_0001.mp4', 'id3_id0_0007.mp4', 'id3_id0_0005.mp4', 'id3_id16_0008.mp4', 'id3_id16_0000.mp4', 'id3_id16_0006.mp4', 'id3_id16_0003.mp4', 'id3_id0_0006.mp4', 'id3_id16_0002.mp4', 'id3_id0_0004.mp4', 'id3_id0_0008.mp4', 'id3_id17_0000.mp4', 'id3_id16_0009.mp4', 'id3_id16_0004.mp4', 'id3_id16_0005.mp4', 'id3_id0_0009.mp4', 'id3_id16_0001.mp4', 'id3_id17_0002.mp4', 'id3_id17_0003.mp4', 'id3_id1_0004.mp4', 'id3_id17_0004.mp4', 'id3_id1_0005.mp4', 'id3_id1_0007.mp4', 'id3_id17_0007.mp4', 'id3_id1_0002.mp4', 'id3_id2_0002.mp4', 'id3_id1_0008.mp4', 'id3_id17_0009.mp4', 'id3_id2_0001.mp4', 'id3_id17_0008.mp4', 'id3_id1_0009.mp4', 'id3_id1_0003.mp4', 'id3_id2_0000.mp4', 'id3_id1_0006.mp4', 'id3_id1_0001.mp4', 'id3_id17_0005.mp4', 'id3_id17_0006.mp4', 'id3_id2_0003.mp4', 'id3_id2_0008.mp4', 'id3_id2_0009.mp4', 'id3_id2_0005.mp4', 'id3_id2_0006.mp4', 'id3_id4_0001.mp4', 'id3_id2_0007.mp4', 'id3_id2_0004.mp4', 'id3_id4_0003.mp4', 'id3_id4_0004.mp4', 'id3_id4_0006.mp4', 'id3_id6_0002.mp4', 'id3_id6_0007.mp4', 'id3_id6_0005.mp4', 'id3_id6_0003.mp4', 'id3_id6_0001.mp4', 'id3_id6_0000.mp4', 'id3_id4_0008.mp4', 'id3_id4_0005.mp4', 'id3_id6_0006.mp4', 'id3_id6_0008.mp4', 'id3_id6_0004.mp4', 'id3_id9_0002.mp4', 'id3_id6_0009.mp4', 'id4_id0_0000.mp4', 'id4_id0_0001.mp4', 'id3_id9_0007.mp4', 'id3_id9_0009.mp4', 'id3_id9_0000.mp4', 'id3_id9_0008.mp4', 'id3_id9_0005.mp4', 'id3_id9_0004.mp4', 'id3_id9_0001.mp4', 'id3_id9_0003.mp4', 'id3_id9_0006.mp4', 'id4_id0_0007.mp4', 'id4_id0_0002.mp4', 'id4_id1_0007.mp4', 'id4_id1_0003.mp4', 'id4_id0_0006.mp4', 'id4_id1_0005.mp4', 'id4_id1_0006.mp4', 'id4_id0_0003.mp4', 'id4_id1_0009.mp4', 'id4_id1_0001.mp4', 'id4_id0_0008.mp4', 'id4_id0_0005.mp4', 'id4_id1_0002.mp4', 'id4_id2_0000.mp4', 'id4_id0_0009.mp4', 'id4_id1_0008.mp4', 'id4_id0_0004.mp4', 'id4_id1_0000.mp4', 'id4_id1_0004.mp4', 'id4_id3_0005.mp4', 'id4_id2_0001.mp4', 'id4_id3_0003.mp4', 'id4_id2_0008.mp4', 'id4_id3_0001.mp4', 'id4_id2_0009.mp4', 'id4_id2_0002.mp4', 'id4_id2_0007.mp4', 'id4_id3_0006.mp4', 'id4_id2_0006.mp4', 'id4_id2_0004.mp4', 'id4_id2_0003.mp4', 'id4_id3_0007.mp4', 'id4_id3_0004.mp4', 'id4_id3_0002.mp4', 'id4_id3_0000.mp4', 'id4_id2_0005.mp4', 'id4_id6_0003.mp4', 'id4_id6_0000.mp4', 'id4_id6_0001.mp4', 'id4_id6_0008.mp4', 'id4_id3_0008.mp4', 'id4_id6_0009.mp4', 'id4_id6_0002.mp4', 'id4_id6_0005.mp4', 'id4_id3_0009.mp4', 'id4_id6_0007.mp4', 'id4_id6_0004.mp4', 'id4_id6_0006.mp4', 'id4_id9_0008.mp4', 'id4_id9_0006.mp4', 'id4_id9_0002.mp4', 'id4_id9_0007.mp4', 'id4_id9_0004.mp4', 'id4_id9_0000.mp4', 'id4_id9_0001.mp4', 'id6_id0_0003.mp4', 'id6_id0_0002.mp4', 'id6_id0_0001.mp4', 'id6_id0_0007.mp4', 'id6_id0_0000.mp4', 'id4_id9_0003.mp4', 'id6_id0_0006.mp4', 'id6_id0_0005.mp4', 'id6_id0_0004.mp4', 'id4_id9_0005.mp4', 'id6_id16_0001.mp4', 'id6_id0_0009.mp4', 'id6_id16_0005.mp4', 'id6_id16_0003.mp4', 'id6_id16_0008.mp4', 'id6_id16_0000.mp4', 'id6_id16_0007.mp4', 'id6_id16_0002.mp4', 'id6_id16_0006.mp4', 'id6_id17_0002.mp4', 'id6_id16_0004.mp4', 'id6_id0_0008.mp4', 'id6_id17_0000.mp4', 'id6_id17_0001.mp4', 'id6_id16_0009.mp4', 'id6_id17_0006.mp4', 'id6_id1_0002.mp4', 'id6_id1_0003.mp4', 'id6_id17_0005.mp4', 'id6_id17_0004.mp4', 'id6_id17_0007.mp4', 'id6_id17_0003.mp4', 'id6_id17_0009.mp4', 'id6_id1_0000.mp4', 'id6_id17_0008.mp4', 'id6_id1_0001.mp4', 'id6_id1_0007.mp4', 'id6_id2_0001.mp4', 'id6_id1_0009.mp4', 'id6_id1_0004.mp4', 'id6_id1_0005.mp4', 'id6_id2_0003.mp4', 'id6_id2_0007.mp4', 'id6_id2_0008.mp4', 'id6_id2_0005.mp4', 'id6_id1_0006.mp4', 'id6_id2_0000.mp4', 'id6_id2_0002.mp4', 'id6_id2_0004.mp4', 'id6_id2_0006.mp4', 'id6_id1_0008.mp4', 'id6_id4_0006.mp4', 'id6_id4_0007.mp4', 'id6_id3_0009.mp4', 'id6_id3_0008.mp4', 'id6_id4_0004.mp4', 'id6_id3_0003.mp4', 'id6_id3_0004.mp4', 'id6_id3_0002.mp4', 'id6_id3_0001.mp4', 'id6_id4_0001.mp4', 'id6_id3_0000.mp4', 'id6_id3_0007.mp4', 'id6_id2_0009.mp4', 'id6_id3_0006.mp4', 'id6_id4_0005.mp4', 'id6_id3_0005.mp4', 'id6_id9_0006.mp4', 'id6_id4_0008.mp4', 'id6_id9_0005.mp4', 'id6_id9_0008.mp4', 'id6_id9_0004.mp4', 'id6_id9_0000.mp4', 'id6_id9_0001.mp4', 'id6_id9_0007.mp4', 'id6_id9_0003.mp4', 'id6_id9_0002.mp4', 'id6_id4_0009.mp4', 'id7_id10_0002.mp4', 'id7_id11_0009.mp4', 'id7_id10_0003.mp4', 'id7_id12_0000.mp4', 'id7_id11_0001.mp4', 'id7_id10_0007.mp4', 'id7_id10_0009.mp4', 'id7_id11_0002.mp4', 'id7_id10_0001.mp4', 'id7_id11_0005.mp4', 'id7_id10_0004.mp4', 'id7_id11_0000.mp4', 'id7_id10_0005.mp4', 'id7_id11_0006.mp4', 'id7_id11_0004.mp4', 'id7_id11_0007.mp4', 'id6_id9_0009.mp4', 'id7_id13_0002.mp4', 'id7_id12_0005.mp4', 'id7_id13_0006.mp4', 'id8_id1_0002.mp4', 'id7_id13_0004.mp4', 'id8_id0_0003.mp4', 'id7_id12_0009.mp4', 'id7_id12_0004.mp4', 'id8_id0_0007.mp4', 'id8_id0_0002.mp4', 'id7_id12_0002.mp4', 'id7_id13_0000.mp4', 'id7_id12_0007.mp4', 'id7_id13_0009.mp4', 'id7_id13_0005.mp4', 'id7_id13_0007.mp4', 'id8_id1_0003.mp4', 'id8_id0_0008.mp4', 'id7_id13_0001.mp4', 'id8_id1_0007.mp4', 'id8_id1_0008.mp4', 'id8_id3_0007.mp4', 'id8_id2_0008.mp4', 'id8_id2_0002.mp4', 'id8_id2_0003.mp4', 'id8_id3_0003.mp4', 'id8_id3_0008.mp4', 'id8_id3_0002.mp4', 'id8_id2_0007.mp4', 'id8_id5_0002.mp4', 'id8_id7_0007.mp4', 'id8_id6_0008.mp4', 'id8_id5_0003.mp4', 'id8_id5_0007.mp4', 'id8_id6_0002.mp4', 'id8_id7_0002.mp4', 'id8_id6_0003.mp4', 'id8_id5_0008.mp4', 'id8_id4_0007.mp4', 'id8_id7_0003.mp4', 'id8_id6_0007.mp4', 'id8_id7_0008.mp4', 'id8_id9_0002.mp4', 'id9_id16_0003.mp4', 'id9_id16_0004.mp4', 'id9_id0_0001.mp4', 'id9_id0_0005.mp4', 'id9_id0_0009.mp4', 'id9_id0_0003.mp4', 'id9_id16_0000.mp4', 'id8_id9_0008.mp4', 'id9_id0_0006.mp4', 'id8_id9_0003.mp4', 'id9_id0_0007.mp4', 'id9_id16_0001.mp4', 'id9_id0_0000.mp4', 'id9_id0_0004.mp4', 'id8_id9_0007.mp4', 'id9_id16_0002.mp4', 'id9_id0_0008.mp4', 'id9_id16_0006.mp4', 'id9_id16_0008.mp4', 'id9_id16_0005.mp4', 'id9_id16_0007.mp4', 'id9_id17_0000.mp4', 'id9_id17_0003.mp4', 'id9_id17_0001.mp4', 'id9_id17_0004.mp4', 'id9_id17_0002.mp4', 'id9_id16_0009.mp4', 'id9_id17_0005.mp4', 'id9_id2_0000.mp4', 'id9_id17_0006.mp4', 'id9_id1_0000.mp4', 'id9_id1_0001.mp4', 'id9_id2_0001.mp4', 'id9_id1_0009.mp4', 'id9_id1_0006.mp4', 'id9_id17_0008.mp4', 'id9_id1_0004.mp4', 'id9_id2_0002.mp4', 'id9_id17_0009.mp4', 'id9_id17_0007.mp4', 'id9_id1_0002.mp4', 'id9_id1_0008.mp4', 'id9_id1_0005.mp4', 'id9_id2_0009.mp4', 'id9_id2_0005.mp4', 'id9_id3_0007.mp4', 'id9_id4_0001.mp4', 'id9_id3_0000.mp4', 'id9_id3_0008.mp4', 'id9_id3_0004.mp4', 'id9_id3_0001.mp4', 'id9_id2_0008.mp4', 'id9_id3_0009.mp4', 'id9_id2_0004.mp4', 'id9_id2_0007.mp4', 'id9_id3_0006.mp4', 'id9_id3_0002.mp4', 'id9_id4_0000.mp4', 'id9_id3_0005.mp4', 'id9_id2_0006.mp4', 'id9_id4_0008.mp4', 'id9_id4_0009.mp4', 'id9_id4_0006.mp4', 'id9_id6_0001.mp4', 'id9_id6_0002.mp4', 'id9_id4_0007.mp4', 'id9_id4_0004.mp4', 'id9_id4_0002.mp4', 'id9_id6_0004.mp4', 'id9_id4_0005.mp4', 'id9_id6_0000.mp4', 'id9_id6_0005.mp4', 'id9_id6_0009.mp4', 'id9_id6_0007.mp4', 'id9_id6_0008.mp4', 'id9_id6_0006.mp4']\n"
          ]
        }
      ],
      "source": [
        "# Define the paths to your dataset in Google Drive\n",
        "real_videos_dir = '/content/drive/MyDrive/Celeb-DF/Celeb-real'\n",
        "fake_videos_dir = '/content/drive/MyDrive/Celeb-DF/Celeb-synthesis'\n",
        "\n",
        "# Verify that the paths are correct\n",
        "import os\n",
        "print(os.listdir(real_videos_dir))\n",
        "print(os.listdir(fake_videos_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KORsgpYQqyX5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "# Output paths for frames\n",
        "real_frames_path = '/content/drive/MyDrive/Celeb-DF/Frames-Real'\n",
        "fake_frames_path = '/content/drive/MyDrive/Celeb-DF/Frames-Fake'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYzW-Yi4qyV0",
        "outputId": "a532b953-851d-4b31-c921-ddfbb2be8932"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmented 2538 real frames.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define minimal data augmentation for real frames\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=5,\n",
        "    width_shift_range=0.05,\n",
        "    height_shift_range=0.05,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Augment frames without saving\n",
        "def augment_frames(image_folder):\n",
        "    augmented_frames = []\n",
        "    for filename in os.listdir(image_folder):\n",
        "        image_path = os.path.join(image_folder, filename)\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "        image = image.reshape((1,) + image.shape)  # Reshape for datagen\n",
        "        augmented_iter = datagen.flow(image, batch_size=1)\n",
        "        augmented_frames.append(next(augmented_iter)[0].astype(np.uint8))  # Take one augmented frame\n",
        "    return augmented_frames\n",
        "\n",
        "# Load and augment real frames only\n",
        "augmented_real_frames = augment_frames(real_frames_path)\n",
        "print(f\"Augmented {len(augmented_real_frames)} real frames.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def load_and_process_fake_frames(fake_frames_path, batch_size=32):\n",
        "#     \"\"\"Loads and processes fake frames in batches.\"\"\"\n",
        "#     filenames = os.listdir(fake_frames_path)\n",
        "#     num_batches = (len(filenames) + batch_size - 1) // batch_size\n",
        "\n",
        "#     for batch_idx in range(num_batches):\n",
        "#         start_idx = batch_idx * batch_size\n",
        "#         end_idx = min((batch_idx + 1) * batch_size, len(filenames))\n",
        "#         batch_filenames = filenames[start_idx:end_idx]\n",
        "\n",
        "#         batch_frames = []\n",
        "#         for filename in batch_filenames:\n",
        "#             image_path = os.path.join(fake_frames_path, filename)\n",
        "#             image = cv2.imread(image_path)\n",
        "#             image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "#             image = cv2.resize(image, (256, 256))\n",
        "#             batch_frames.append(image)\n",
        "\n",
        "#         # Convert batch to numpy array\n",
        "#         batch_frames = np.array(batch_frames)\n",
        "\n",
        "# # Call the function to load and process frames in batches\n",
        "# load_and_process_fake_frames(fake_frames_path)"
      ],
      "metadata": {
        "id": "q_g8b05K0SVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xpk5R5HPqyUN",
        "outputId": "0275bdd9-888b-49d5-815c-2ff5f858d9c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 12888 fake frames.\n"
          ]
        }
      ],
      "source": [
        "# Load fake frames\n",
        "fake_frames = [cv2.cvtColor(cv2.imread(os.path.join(fake_frames_path, filename)), cv2.COLOR_BGR2RGB) for filename in os.listdir(fake_frames_path)]\n",
        "print(f\"Loaded {len(fake_frames)} fake frames.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qfz-BRpxspgc"
      },
      "outputs": [],
      "source": [
        "# Resize fake frames before converting to numpy array\n",
        "fake_frames = [cv2.resize(frame, (256, 256)) for frame in fake_frames]\n",
        "\n",
        "# Ensure all augmented real frames are the same size\n",
        "augmented_real_frames = [cv2.resize(frame, (256, 256)) for frame in augmented_real_frames]\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "augmented_real_frames = np.array(augmented_real_frames)\n",
        "fake_frames = np.array(fake_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0VfIdJYwvTv"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, LayerNormalization, Add, Input, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define a simplified PVT block\n",
        "def pvt_block(x, num_channels):\n",
        "    shortcut = x\n",
        "    # Initialize layers outside the function to avoid creating them during tracing\n",
        "    conv1 = Conv2D(num_channels, kernel_size=1, strides=1, padding='same')\n",
        "    norm1 = LayerNormalization()\n",
        "    conv2 = Conv2D(num_channels, kernel_size=3, strides=1, padding='same')\n",
        "    norm2 = LayerNormalization()\n",
        "    proj = Conv2D(num_channels * 4, kernel_size=1, strides=1, padding='same')\n",
        "    conv3 = Conv2D(num_channels * 4, kernel_size=1, strides=1, padding='same')\n",
        "    norm3 = LayerNormalization()\n",
        "    add = Add()\n",
        "\n",
        "    x = conv1(x)  # Use the pre-initialized layers\n",
        "    x = norm1(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = conv2(x)\n",
        "    x = norm2(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    # Project the shortcut to match the number of channels\n",
        "    shortcut = proj(shortcut)\n",
        "    x = conv3(x)\n",
        "    x = norm3(x)\n",
        "    x = add([shortcut, x])\n",
        "    x = tf.nn.relu(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnWV9Lurxzp3",
        "outputId": "86ff2845-3e2a-4b16-8837-1b05d46be5d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)             (None, 224, 224, 8)          32        ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization (Layer  (None, 224, 224, 8)          16        ['conv2d[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " tf.nn.relu (TFOpLambda)     (None, 224, 224, 8)          0         ['layer_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)           (None, 224, 224, 8)          584       ['tf.nn.relu[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_1 (Lay  (None, 224, 224, 8)          16        ['conv2d_1[0][0]']            \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " tf.nn.relu_1 (TFOpLambda)   (None, 224, 224, 8)          0         ['layer_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)           (None, 224, 224, 32)         288       ['tf.nn.relu_1[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)           (None, 224, 224, 32)         128       ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_2 (Lay  (None, 224, 224, 32)         64        ['conv2d_3[0][0]']            \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " add (Add)                   (None, 224, 224, 32)         0         ['conv2d_2[0][0]',            \n",
            "                                                                     'layer_normalization_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " tf.nn.relu_2 (TFOpLambda)   (None, 224, 224, 32)         0         ['add[0][0]']                 \n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 1605632)              0         ['tf.nn.relu_2[0][0]']        \n",
            "                                                                                                  \n",
            " binary_output (Dense)       (None, 1)                    1605633   ['flatten[0][0]']             \n",
            "                                                                                                  \n",
            " distillation_output (Dense  (None, 1)                    1605633   ['flatten[0][0]']             \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " multimodal_output (Dense)   (None, 1)                    1605633   ['flatten[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4818027 (18.38 MB)\n",
            "Trainable params: 4818027 (18.38 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, LayerNormalization, Add, Input, Flatten, Dense # Import Flatten and Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define the model\n",
        "input_shape = (224, 224, 3)  # Adjust this based on your frame size\n",
        "inputs = Input(shape=input_shape)\n",
        "x = pvt_block(inputs, num_channels=8)\n",
        "# Add more PVT blocks or layers as needed\n",
        "\n",
        "x = Flatten()(x)\n",
        "binary_output = Dense(1, activation='sigmoid', name='binary_output')(x)\n",
        "distillation_output = Dense(1, activation='sigmoid', name='distillation_output')(x)\n",
        "multimodal_output = Dense(1, activation='sigmoid', name='multimodal_output')(x)\n",
        "\n",
        "modelrgb = Model(inputs, [binary_output, distillation_output, multimodal_output])\n",
        "\n",
        "# Compile the model with multiple loss functions and metrics\n",
        "modelrgb.compile(\n",
        "    optimizer='adam',\n",
        "    loss={'binary_output': 'binary_crossentropy', 'distillation_output': 'binary_crossentropy', 'multimodal_output': 'binary_crossentropy'},\n",
        "    metrics={'binary_output': 'accuracy', 'distillation_output': 'accuracy', 'multimodal_output': 'accuracy'}\n",
        ")\n",
        "\n",
        "# Display the model summary\n",
        "modelrgb.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyVy9D8bsDjg"
      },
      "outputs": [],
      "source": [
        "# Prepare the dataset\n",
        "X = np.concatenate((augmented_real_frames, fake_frames))\n",
        "# Resize all frames to match model input\n",
        "X = np.array([cv2.resize(frame, (224, 224)) for frame in X])\n",
        "y = np.concatenate((np.ones(len(augmented_real_frames)), np.zeros(len(fake_frames))))\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmlkiE4BuPN_",
        "outputId": "0210397e-2ee6-4ec1-800e-948b561a247d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "617/617 [==============================] - 160s 248ms/step - loss: 1060.4971 - binary_output_loss: 270.9855 - distillation_output_loss: 369.6587 - multimodal_output_loss: 419.8534 - binary_output_accuracy: 0.8156 - distillation_output_accuracy: 0.8065 - multimodal_output_accuracy: 0.8070 - val_loss: 126.6028 - val_binary_output_loss: 25.8935 - val_distillation_output_loss: 76.0532 - val_multimodal_output_loss: 24.6562 - val_binary_output_accuracy: 0.9088 - val_distillation_output_accuracy: 0.8744 - val_multimodal_output_accuracy: 0.9092\n",
            "Epoch 2/3\n",
            "617/617 [==============================] - 155s 251ms/step - loss: 48.5134 - binary_output_loss: 15.5819 - distillation_output_loss: 17.1664 - multimodal_output_loss: 15.7651 - binary_output_accuracy: 0.9109 - distillation_output_accuracy: 0.9131 - multimodal_output_accuracy: 0.9113 - val_loss: 22.7642 - val_binary_output_loss: 7.7471 - val_distillation_output_loss: 8.3607 - val_multimodal_output_loss: 6.6564 - val_binary_output_accuracy: 0.9153 - val_distillation_output_accuracy: 0.9323 - val_multimodal_output_accuracy: 0.9214\n",
            "Epoch 3/3\n",
            "617/617 [==============================] - 154s 250ms/step - loss: 12.4700 - binary_output_loss: 4.7333 - distillation_output_loss: 4.2755 - multimodal_output_loss: 3.4611 - binary_output_accuracy: 0.9490 - distillation_output_accuracy: 0.9469 - multimodal_output_accuracy: 0.9533 - val_loss: 17.8928 - val_binary_output_loss: 5.5026 - val_distillation_output_loss: 6.6279 - val_multimodal_output_loss: 5.7623 - val_binary_output_accuracy: 0.9250 - val_distillation_output_accuracy: 0.9425 - val_multimodal_output_accuracy: 0.9242\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c68a4102860>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Train the model\n",
        "modelrgb.fit(X_train, y_train, epochs=3, batch_size=16, validation_split=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjBB0JcNEB_K",
        "outputId": "1ee230ad-f91b-4c1b-9ceb-cf6c8c3a80f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "386/386 [==============================] - 81s 209ms/step\n",
            "97/97 [==============================] - 20s 209ms/step\n"
          ]
        }
      ],
      "source": [
        "# Get predictions for training data\n",
        "predictions_train = modelrgb.predict(X_train)\n",
        "distillation_predictions_train = predictions_train[1] # Access the second element of the list, which corresponds to distillation output\n",
        "multimodal_predictions_train = predictions_train[2]   # Access the third element, which corresponds to multimodal output\n",
        "\n",
        "# Get predictions for testing data\n",
        "predictions_test = modelrgb.predict(X_test)\n",
        "distillation_predictions_test = predictions_test[1]\n",
        "multimodal_predictions_test = predictions_test[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAbAxHNCuTL7",
        "outputId": "24577aaa-c38a-4b68-e2ad-2637cd6af16b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "97/97 [==============================] - 20s 210ms/step - loss: 11.1751 - binary_output_loss: 2.8963 - distillation_output_loss: 4.1040 - multimodal_output_loss: 4.1748 - binary_output_accuracy: 0.9300 - distillation_output_accuracy: 0.9494 - multimodal_output_accuracy: 0.9417\n",
            "Model accuracy: 94.94%\n"
          ]
        }
      ],
      "source": [
        "# Assuming you have already split your data into X_test and y_test\n",
        "\n",
        "# Evaluate the model on test data\n",
        "accuracy = modelrgb.evaluate(X_test, y_test)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Model accuracy: {accuracy[5] * 100:.2f}%\")  # Assuming accuracy is the 5th metric in your model's metrics list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxmuhsfFERLs",
        "outputId": "9abe99d0-89f5-4685-e2e6-920cf45c952b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "97/97 [==============================] - 20s 210ms/step - loss: 11.1751 - binary_output_loss: 2.8963 - distillation_output_loss: 4.1040 - multimodal_output_loss: 4.1748 - binary_output_accuracy: 0.9300 - distillation_output_accuracy: 0.9494 - multimodal_output_accuracy: 0.9417\n",
            "Model evaluation results:\n",
            "Loss: 11.175082206726074\n",
            "Binary Output Accuracy: 94.94%\n",
            "Distillation Output Accuracy: 94.17%\n",
            "Multimodal Output Accuracy: 94.17%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Evaluate the model on test data\n",
        "accuracy = modelrgb.evaluate(X_test, y_test)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(f\"Model evaluation results:\")\n",
        "print(f\"Loss: {accuracy[0]}\")\n",
        "print(f\"Binary Output Accuracy: {accuracy[5] * 100:.2f}%\")\n",
        "print(f\"Distillation Output Accuracy: {accuracy[6] * 100:.2f}%\")\n",
        "print(f\"Multimodal Output Accuracy: {accuracy[6] * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x62XI-Sm8nuB"
      },
      "source": [
        "# SECOND PIPELINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWfy1O30AWCA",
        "outputId": "43c585ab-479a-4024-f30a-756c15f56e0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/librosa/feature/spectral.py:2143: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
            "  mel_basis = filters.mel(sr=sr, n_fft=n_fft, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted features from 2538 augmented real frames.\n",
            "Extracted features from 12888 fake frames.\n"
          ]
        }
      ],
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "from scipy.fftpack import fft\n",
        "from scipy.signal import butter, lfilter\n",
        "\n",
        "def extract_mfcc_fft(image):\n",
        "    image_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "    signal = image_gray.mean(axis=0)\n",
        "\n",
        "    # High-pass filter\n",
        "    cutoff_freq = 100  # Adjust this value based on your desired cutoff frequency\n",
        "    nyquist_freq = 0.5 * 22050\n",
        "    normal_cutoff = cutoff_freq / nyquist_freq\n",
        "    b, a = butter(4, normal_cutoff, btype='high', analog=False)\n",
        "    filtered_signal = lfilter(b, a, signal)\n",
        "\n",
        "    # Adjust n_fft based on the length of filtered_signal\n",
        "    n_fft = min(2048, len(filtered_signal))  # Set n_fft to be smaller of 2048 or signal length\n",
        "    mfccs = librosa.feature.mfcc(y=filtered_signal, sr=22050, n_mfcc=13, n_fft=n_fft)\n",
        "    fft_features = np.abs(fft(filtered_signal, n=n_fft))\n",
        "    return mfccs, fft_features\n",
        "\n",
        "# Example usage\n",
        "# real_features = [extract_mfcc_fft(frame) for frame in augmented_real_frames]\n",
        "# fake_features = [extract_mfcc_fft(frame) for frame in fake_frames]\n",
        "\n",
        "# Ensure to handle varying signal lengths appropriately in your actual application.\n",
        "\n",
        "\n",
        "# Extract features from augmented real frames\n",
        "real_features = [extract_mfcc_fft(frame) for frame in augmented_real_frames]\n",
        "\n",
        "# Extract features from fake frames (not augmented)\n",
        "fake_features = []\n",
        "for filename in os.listdir(fake_frames_path):\n",
        "    image_path = os.path.join(fake_frames_path, filename)\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    mfcc, fft_features = extract_mfcc_fft(image)\n",
        "    fake_features.append((mfcc, fft_features))\n",
        "\n",
        "print(f\"Extracted features from {len(real_features)} augmented real frames.\")\n",
        "print(f\"Extracted features from {len(fake_features)} fake frames.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj8PILO3AV-o",
        "outputId": "3518b1df-09b2-4311-f34a-6b48bec1a412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MFCC features shape: (15426, 13, 2)\n",
            "FFT features shape: (15426, 974)\n",
            "Labels shape: (15426,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming you have extracted features stored in real_features and fake_features\n",
        "# real_features and fake_features should be lists of tuples (mfccs, fft_features)\n",
        "\n",
        "# Pad or truncate MFCC features to a consistent length\n",
        "max_length_mfcc = max(len(f[0][0]) for f in real_features + fake_features)  # Find the maximum length\n",
        "X_mfcc = []\n",
        "for f in real_features + fake_features:\n",
        "    mfcc = f[0]\n",
        "    if len(mfcc[0]) < max_length_mfcc:\n",
        "        # Pad with zeros along the time axis\n",
        "        mfcc = np.pad(mfcc, ((0, 0), (0, max_length_mfcc - len(mfcc[0]))), mode='constant')\n",
        "    elif len(mfcc[0]) > max_length_mfcc:\n",
        "        # Truncate along the time axis\n",
        "        mfcc = mfcc[:, :max_length_mfcc]\n",
        "    X_mfcc.append(mfcc)\n",
        "\n",
        "X_mfcc = np.array(X_mfcc)\n",
        "\n",
        "# Pad or truncate FFT features to a consistent length\n",
        "max_length_fft = max(len(f[1]) for f in real_features + fake_features)\n",
        "X_fft = []\n",
        "for f in real_features + fake_features:\n",
        "    fft_features = f[1]\n",
        "    if len(fft_features) < max_length_fft:\n",
        "        # Pad with zeros\n",
        "        fft_features = np.pad(fft_features, (0, max_length_fft - len(fft_features)), mode='constant')\n",
        "    elif len(fft_features) > max_length_fft:\n",
        "        # Truncate\n",
        "        fft_features = fft_features[:max_length_fft]\n",
        "    X_fft.append(fft_features)\n",
        "\n",
        "X_fft = np.array(X_fft)\n",
        "\n",
        "# Create labels: 0 for real, 1 for fake\n",
        "y_labels = np.array([0] * len(real_features) + [1] * len(fake_features))\n",
        "\n",
        "print(f\"MFCC features shape: {X_mfcc.shape}\")\n",
        "print(f\"FFT features shape: {X_fft.shape}\")\n",
        "print(f\"Labels shape: {y_labels.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ny0SEgmAIWiT"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2vvSJ1dsAV9B"
      },
      "outputs": [],
      "source": [
        "!pip install vit-keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lq_L1RNqJC0m"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input\n",
        "import numpy as np\n",
        "\n",
        "# Assuming X_mfcc and X_fft are already defined\n",
        "input_mfcc = Input(shape=X_mfcc.shape[1:], name='input_mfcc')\n",
        "input_fft = Input(shape=X_fft.shape[1:], name='input_fft')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WpnN4ep2JODh"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade tensorflow\n",
        "!pip install tensorflow tensorflow-addons vit-keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjvKYl6mqJZX",
        "outputId": "ddd25187-e55a-4a2b-838e-6661551582ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of input_distillation_second: (None, 1)\n",
            "Shape of input_multimodal_second: (None, 1)\n",
            "Shape of vision_transformer_output: (None, 1)\n",
            "Shape of input_mfcc: KerasTensor(type_spec=TensorSpec(shape=(3,), dtype=tf.int32, name=None), inferred_value=[None, 13, 2], name='tf.compat.v1.shape/Shape:0', description=\"created by layer 'tf.compat.v1.shape'\")\n",
            "Shape of input_fft: KerasTensor(type_spec=TensorSpec(shape=(2,), dtype=tf.int32, name=None), inferred_value=[None, 974], name='tf.compat.v1.shape_1/Shape:0', description=\"created by layer 'tf.compat.v1.shape_1'\")\n",
            "Shape of input_distillation_second: KerasTensor(type_spec=TensorSpec(shape=(2,), dtype=tf.int32, name=None), inferred_value=[None, 1], name='tf.compat.v1.shape_2/Shape:0', description=\"created by layer 'tf.compat.v1.shape_2'\")\n",
            "Shape of input_multimodal_second: KerasTensor(type_spec=TensorSpec(shape=(2,), dtype=tf.int32, name=None), inferred_value=[None, 1], name='tf.compat.v1.shape_3/Shape:0', description=\"created by layer 'tf.compat.v1.shape_3'\")\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_fft (InputLayer)      [(None, 974)]                0         []                            \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 256)                  249600    ['input_fft[0][0]']           \n",
            "                                                                                                  \n",
            " input_mfcc (InputLayer)     [(None, 13, 2)]              0         []                            \n",
            "                                                                                                  \n",
            " reshape (Reshape)           (None, 1, 256)               0         ['dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 13, 256)              768       ['input_mfcc[0][0]']          \n",
            "                                                                                                  \n",
            " reshape_2 (Reshape)         (None, 1, 256)               0         ['reshape[0][0]']             \n",
            "                                                                                                  \n",
            " tf.concat_1 (TFOpLambda)    (None, 14, 256)              0         ['dense[0][0]',               \n",
            "                                                                     'reshape_2[0][0]']           \n",
            "                                                                                                  \n",
            " reshape_3 (Reshape)         (None, 14, 256)              0         ['tf.concat_1[0][0]']         \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 14, 256)              65792     ['reshape_3[0][0]']           \n",
            "                                                                                                  \n",
            " layer_normalization_3 (Lay  (None, 14, 256)              512       ['dense_2[0][0]']             \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " tf.nn.relu_3 (TFOpLambda)   (None, 14, 256)              0         ['layer_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 14, 256)              65792     ['tf.nn.relu_3[0][0]']        \n",
            "                                                                                                  \n",
            " layer_normalization_4 (Lay  (None, 14, 256)              512       ['dense_3[0][0]']             \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " tf.nn.relu_4 (TFOpLambda)   (None, 14, 256)              0         ['layer_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 14, 1024)             263168    ['tf.nn.relu_4[0][0]']        \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 14, 1024)             263168    ['reshape_3[0][0]']           \n",
            "                                                                                                  \n",
            " layer_normalization_5 (Lay  (None, 14, 1024)             2048      ['dense_5[0][0]']             \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 14, 1024)             0         ['dense_4[0][0]',             \n",
            "                                                                     'layer_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " tf.nn.relu_5 (TFOpLambda)   (None, 14, 1024)             0         ['add_1[0][0]']               \n",
            "                                                                                                  \n",
            " multi_head_attention (Mult  (None, 14, 1024)             8395776   ['tf.nn.relu_5[0][0]',        \n",
            " iHeadAttention)                                                     'tf.nn.relu_5[0][0]']        \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 14, 1024)             0         ['multi_head_attention[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOp  (None, 14, 1024)             0         ['dropout[0][0]',             \n",
            " Lambda)                                                             'tf.nn.relu_5[0][0]']        \n",
            "                                                                                                  \n",
            " layer_normalization_6 (Lay  (None, 14, 1024)             2048      ['tf.__operators__.add[0][0]']\n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)             (None, 14, 512)              524800    ['layer_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)           (None, 14, 256)              131328    ['conv1d[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 14, 256)              0         ['conv1d_1[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)           (None, 14, 256)              262400    ['layer_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TF  (None, 14, 256)              0         ['dropout_1[0][0]',           \n",
            " OpLambda)                                                           'conv1d_2[0][0]']            \n",
            "                                                                                                  \n",
            " layer_normalization_7 (Lay  (None, 14, 256)              512       ['tf.__operators__.add_1[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " flatten_4 (Flatten)         (None, 3584)                 0         ['layer_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " vision_transformer_output   (None, 1)                    3585      ['flatten_4[0][0]']           \n",
            " (Dense)                                                                                          \n",
            "                                                                                                  \n",
            " input_distillation_second   [(None, 1)]                  0         []                            \n",
            " (InputLayer)                                                                                     \n",
            "                                                                                                  \n",
            " input_multimodal_second (I  [(None, 1)]                  0         []                            \n",
            " nputLayer)                                                                                       \n",
            "                                                                                                  \n",
            " flatten_5 (Flatten)         (None, 1)                    0         ['vision_transformer_output[0]\n",
            "                                                                    [0]']                         \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate  (None, 3)                    0         ['input_distillation_second[0]\n",
            " )                                                                  [0]',                         \n",
            "                                                                     'input_multimodal_second[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'flatten_5[0][0]']           \n",
            "                                                                                                  \n",
            " flatten_6 (Flatten)         (None, 3)                    0         ['concatenate_1[0][0]']       \n",
            "                                                                                                  \n",
            " output (Dense)              (None, 2)                    8         ['flatten_6[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 10231817 (39.03 MB)\n",
            "Trainable params: 10231817 (39.03 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, LayerNormalization, Reshape, Add, MultiHeadAttention, Dropout, Concatenate, Conv1D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define input shapes for MFCC and FFT features\n",
        "input_mfcc_shape = (13, 2)  # MFCC shape: 13 coefficients, 2 time steps (assuming 2 channels)\n",
        "input_fft_shape = (974,)  # FFT shape: 974 features\n",
        "\n",
        "# Inputs for MFCC and FFT features\n",
        "input_mfcc = Input(shape=input_mfcc_shape, name='input_mfcc')\n",
        "input_fft = Input(shape=input_fft_shape, name='input_fft')\n",
        "\n",
        "flattened_mfcc = Flatten()(input_mfcc)\n",
        "flattened_fft = Flatten()(input_fft)\n",
        "\n",
        "# Patch Embedding layer for MFCC and FFT\n",
        "projection_dim = 256  # Adjust as needed\n",
        "patch_mfcc = Dense(projection_dim)(input_mfcc)\n",
        "patch_fft = Dense(projection_dim)(input_fft)\n",
        "\n",
        "\n",
        " #from here\n",
        "# Reshape patch_fft to match the rank of patch_mfcc\n",
        "patch_fft = Reshape((-1, projection_dim))(patch_fft) # Reshape patch_fft to be 3D\n",
        "\n",
        "# Concatenate patches\n",
        "combined_patches = tf.concat([patch_mfcc, patch_fft], axis=1)\n",
        "\n",
        "# Reshape for PVT block\n",
        "combined_patches = Reshape((-1, projection_dim))(combined_patches)\n",
        "\n",
        "#to here\n",
        "\n",
        "#new added from here\n",
        "\n",
        "# Reshape patch_fft to match the rank of patch_mfcc\n",
        "patch_fft = Reshape((-1, projection_dim))(patch_fft)\n",
        "\n",
        "# Concatenate patches\n",
        "combined_patches = tf.concat([patch_mfcc, patch_fft], axis=1)\n",
        "\n",
        "# Reshape for PVT block\n",
        "combined_patches = Reshape((-1, projection_dim))(combined_patches)\n",
        "\n",
        "# Flatten combined_patches before passing to concatenate_1\n",
        "flattened_combined_patches = Flatten()(combined_patches)\n",
        "\n",
        "input_distillation_second = Input(shape=(distillation_predictions_train.shape[1],), name='input_distillation_second')\n",
        "input_multimodal_second = Input(shape=(multimodal_predictions_train.shape[1],), name='input_multimodal_second')\n",
        "\n",
        "# Now pass flattened_combined_patches to concatenate_1\n",
        "concat_layer = Concatenate()([input_distillation_second,\n",
        "                               input_multimodal_second,\n",
        "                               flattened_combined_patches])\n",
        "\n",
        "#to here\n",
        "\n",
        "# Pyramid Vision Transformer (PVT) block\n",
        "def pvt_block(x, num_channels):\n",
        "    shortcut = x\n",
        "    # Projection layers\n",
        "    x = Dense(num_channels)(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = Dense(num_channels)(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    # Project the shortcut to match the number of channels\n",
        "    shortcut = Dense(num_channels * 4)(shortcut)\n",
        "    x = Dense(num_channels * 4)(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Add()([shortcut, x])\n",
        "    x = tf.nn.relu(x)\n",
        "    return x\n",
        "\n",
        "# Apply PVT block\n",
        "pvt_output = pvt_block(combined_patches, num_channels=256)  # Adjust num_channels as needed\n",
        "\n",
        "# Vision Transformer (ViT) block\n",
        "def vit_block(x, num_heads=8, ff_dim=512, dropout_rate=0.1):\n",
        "    # Multi-head self-attention\n",
        "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim)(x, x)\n",
        "    attn_output = Dropout(dropout_rate)(attn_output)\n",
        "    out1 = LayerNormalization(epsilon=1e-6)(attn_output + x)\n",
        "\n",
        "    # Feed Forward Network\n",
        "    ffn = Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(out1)\n",
        "    # Project back to the original dimension\n",
        "    ffn = Conv1D(filters=projection_dim, kernel_size=1)(ffn)\n",
        "    ffn = Dropout(dropout_rate)(ffn)\n",
        "\n",
        "    # Adjust the number of channels in the skip connection to match FFN output\n",
        "    out1 = Conv1D(filters=projection_dim, kernel_size=1)(out1)\n",
        "\n",
        "    out2 = LayerNormalization(epsilon=1e-6)(ffn + out1)\n",
        "    return out2\n",
        "\n",
        "vit_output = vit_block(pvt_output)\n",
        "vision_transformer_output = Dense(1, activation='sigmoid', name='vision_transformer_output')(tf.keras.layers.Flatten()(vit_output))\n",
        "\n",
        "# Define the inputs for the second pipeline\n",
        "#multimodal_input = Input(shape=multimodal_output.shape[1:], name='multimodal_input')\n",
        "#distillation_input = Input(shape=distillation_output.shape[1:], name='distillation_input')\n",
        "\n",
        "input_distillation_second = Input(shape=(distillation_predictions_train.shape[1],), name='input_distillation_second')\n",
        "input_multimodal_second = Input(shape=(multimodal_predictions_train.shape[1],), name='input_multimodal_second')\n",
        "\n",
        "#input_distillation_second = Input(shape=(1,), name='input_distillation_second')\n",
        "#input_multimodal_second = Input(shape=(1,), name='input_multimodal_second')\n",
        "\n",
        "\n",
        "# Print shapes of inputs before concatenation\n",
        "print(\"Shape of input_distillation_second:\", input_distillation_second.shape)\n",
        "print(\"Shape of input_multimodal_second:\", input_multimodal_second.shape)\n",
        "print(\"Shape of vision_transformer_output:\", vision_transformer_output.shape)\n",
        "\n",
        "\n",
        "#added new\n",
        "# Flatten the vision transformer output before concatenation\n",
        "flattened_vision_output = Flatten()(vision_transformer_output)\n",
        "\n",
        "# Concatenate the flattened outputs\n",
        "merged_output = Concatenate()([input_distillation_second,\n",
        "                               input_multimodal_second,\n",
        "                               flattened_vision_output])\n",
        "# Concatenate outputs\n",
        "#merged_output = Concatenate()([input_distillation_second, input_multimodal_second, vision_transformer_output])\n",
        "\n",
        "# Flatten the merged output before passing it to the Dense layer\n",
        "flattened_output = tf.keras.layers.Flatten()(merged_output)\n",
        "\n",
        "# Final classification head\n",
        "final_output = Dense(2, activation='softmax', name='output')(flattened_output)\n",
        "\n",
        "print(\"Shape of input_mfcc:\", tf.shape(input_mfcc))\n",
        "print(\"Shape of input_fft:\", tf.shape(input_fft))\n",
        "print(\"Shape of input_distillation_second:\", tf.shape(input_distillation_second))\n",
        "print(\"Shape of input_multimodal_second:\", tf.shape(input_multimodal_second))\n",
        "\n",
        "# Create model\n",
        "model = Model(inputs=[input_mfcc, input_fft, input_distillation_second, input_multimodal_second], outputs=final_output)\n",
        "\n",
        "# Recompile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fzhTHCn6MJ2"
      },
      "outputs": [],
      "source": [
        "#leave this, above one is woekring\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Input, Dense, LayerNormalization, Reshape, Add, MultiHeadAttention, Dropout, Concatenate, Conv1D, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define input shapes for MFCC and FFT features\n",
        "input_mfcc_shape = (13, 2)  # MFCC shape: 13 coefficients, 2 time steps (assuming 2 channels)\n",
        "input_fft_shape = (974,)  # FFT shape: 974 features\n",
        "\n",
        "# Inputs for MFCC and FFT features\n",
        "input_mfcc = Input(shape=input_mfcc_shape, name='input_mfcc')\n",
        "input_fft = Input(shape=input_fft_shape, name='input_fft')\n",
        "\n",
        "# Patch Embedding layer for MFCC and FFT\n",
        "projection_dim = 256  # Adjust as needed\n",
        "patch_mfcc = Dense(projection_dim)(Flatten()(input_mfcc))\n",
        "patch_fft = Dense(projection_dim)(Flatten()(input_fft))\n",
        "\n",
        "# Concatenate patches\n",
        "combined_patches = tf.concat([patch_mfcc, patch_fft], axis=1)\n",
        "\n",
        "# Reshape for PVT block\n",
        "combined_patches = Reshape((-1, projection_dim))(combined_patches)\n",
        "\n",
        "# Pyramid Vision Transformer (PVT) block\n",
        "def pvt_block(x, num_channels):\n",
        "    shortcut = x\n",
        "    # Projection layers\n",
        "    x = Dense(num_channels)(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = Dense(num_channels)(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    # Project the shortcut to match the number of channels\n",
        "    shortcut = Dense(num_channels * 4)(shortcut)\n",
        "    x = Dense(num_channels * 4)(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Add()([shortcut, x])\n",
        "    x = tf.nn.relu(x)\n",
        "    return x\n",
        "\n",
        "# Apply PVT block\n",
        "pvt_output = pvt_block(combined_patches, num_channels=256)  # Adjust num_channels as needed\n",
        "\n",
        "# Vision Transformer (ViT) block\n",
        "def vit_block(x, num_heads=8, ff_dim=512, dropout_rate=0.1):\n",
        "    # Multi-head self-attention\n",
        "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim)(x, x)\n",
        "    attn_output = Dropout(dropout_rate)(attn_output)\n",
        "    out1 = LayerNormalization(epsilon=1e-6)(attn_output + x)\n",
        "\n",
        "    # Feed Forward Network\n",
        "    ffn = Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(out1)\n",
        "    # Project back to the original dimension\n",
        "    ffn = Conv1D(filters=projection_dim, kernel_size=1)(ffn)\n",
        "    ffn = Dropout(dropout_rate)(ffn)\n",
        "\n",
        "    out2 = LayerNormalization(epsilon=1e-6)(ffn + out1)\n",
        "    return out2\n",
        "\n",
        "vit_output = vit_block(pvt_output)\n",
        "vision_transformer_output = Dense(1, activation='sigmoid', name='vision_transformer_output')(Flatten()(vit_output))\n",
        "\n",
        "# Define the inputs for the second pipeline\n",
        "input_distillation_second = Input(shape=(distillation_predictions_train.shape[1],), name='input_distillation_second')\n",
        "input_multimodal_second = Input(shape=(multimodal_predictions_train.shape[1],), name='input_multimodal_second')\n",
        "\n",
        "# Concatenate outputs\n",
        "merged_output = Concatenate()([input_distillation_second, input_multimodal_second, Flatten()(vision_transformer_output)])\n",
        "\n",
        "# Final classification head\n",
        "final_output = Dense(2, activation='softmax', name='output')(merged_output)\n",
        "\n",
        "# Create model\n",
        "model = Model(inputs=[input_mfcc, input_fft, input_distillation_second, input_multimodal_second], outputs=final_output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Your generator function\n",
        "def combined_generator(image_gen, distillation_data, multimodal_data):\n",
        "    while True:\n",
        "        # Get batch from image generator\n",
        "        try:\n",
        "            image_data, image_labels = next(image_gen)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image batch: {e}\")\n",
        "            continue  # Skip this batch and move to the next\n",
        "\n",
        "        # Generate dummy data for MFCC and FFT (replace with actual data)\n",
        "        batch_size = image_data.shape[0]\n",
        "        mfcc_data = np.zeros((batch_size, 13, 2))  # Dummy MFCC data\n",
        "        fft_data = np.zeros((batch_size, 974))     # Dummy FFT data\n",
        "\n",
        "        # Convert labels to one-hot encoding\n",
        "        image_labels = tf.keras.utils.to_categorical(image_labels, num_classes=2)\n",
        "\n",
        "        # Select a batch of distillation and multimodal data\n",
        "        start_idx = np.random.randint(0, len(distillation_data) - batch_size)\n",
        "        batch_distillation_data = distillation_data[start_idx: start_idx + batch_size]\n",
        "        batch_multimodal_data = multimodal_data[start_idx: start_idx + batch_size]\n",
        "\n",
        "        yield [mfcc_data, fft_data, batch_distillation_data, batch_multimodal_data], image_labels\n",
        "\n",
        "# Define the base directory for your data\n",
        "base_dir = '/content/drive/MyDrive/Celeb-DF/Split-Frames'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'val')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "# Define ImageDataGenerator for data augmentation and normalization\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Create generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),  # Adjust based on your model input size\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "train_steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "val_steps_per_epoch = val_generator.samples // val_generator.batch_size\n",
        "\n",
        "# Combined training generator\n",
        "combined_train_gen = combined_generator(train_generator, distillation_predictions_train, multimodal_predictions_train)\n",
        "\n",
        "# Combined validation generator\n",
        "combined_val_gen = combined_generator(val_generator, distillation_predictions_test, multimodal_predictions_test)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE_0nEgaB96R",
        "outputId": "5199eb55-6ba8-47ae-fc7d-984a55909190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 12571 images belonging to 2 classes.\n",
            "Found 1541 images belonging to 2 classes.\n",
            "Found 992 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(\n",
        "    combined_train_gen,\n",
        "    steps_per_epoch=train_steps_per_epoch,\n",
        "    epochs=2,  # Adjust based on your requirements\n",
        "    validation_data=combined_val_gen,\n",
        "    validation_steps=val_steps_per_epoch\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQ640u0UB_nU",
        "outputId": "51e3d6b9-921c-44ea-dd3f-087f2700d768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "215/392 [===============>..............] - ETA: 10:59 - loss: 0.6063 - accuracy: 0.7641Error loading image batch: cannot identify image file <_io.BytesIO object at 0x7c67dc2bc400>\n",
            "325/392 [=======================>......] - ETA: 3:50 - loss: 0.5852 - accuracy: 0.7816"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access validation accuracy\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "print(\"Validation Accuracy:\", val_accuracy)"
      ],
      "metadata": {
        "id": "0PQB5zv0FUJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOHshWfV3Fjn"
      },
      "outputs": [],
      "source": [
        "!pip install split-folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YO1J27lmDMAw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "real_frames_path = '/content/drive/MyDrive/Celeb-DF/Frames-Real'\n",
        "fake_frames_path = '/content/drive/MyDrive/Celeb-DF/Frames-Fake'\n",
        "\n",
        "output_base_path = '/content/drive/MyDrive/Celeb-DF/Split-Frames'\n",
        "train_real_path = os.path.join(output_base_path, 'train', 'real')\n",
        "val_real_path = os.path.join(output_base_path, 'val', 'real')\n",
        "test_real_path = os.path.join(output_base_path, 'test', 'real')\n",
        "train_fake_path = os.path.join(output_base_path, 'train', 'fake')\n",
        "val_fake_path = os.path.join(output_base_path, 'val', 'fake')\n",
        "test_fake_path = os.path.join(output_base_path, 'test', 'fake')\n",
        "\n",
        "# Create directories if they don't exist\n",
        "for folder in [train_real_path, val_real_path, test_real_path, train_fake_path, val_fake_path, test_fake_path]:\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aPKJB24A8Mb"
      },
      "outputs": [],
      "source": [
        "def split_and_copy_images(source_folder, train_folder, val_folder, test_folder):\n",
        "    all_images = [f for f in os.listdir(source_folder) if os.path.isfile(os.path.join(source_folder, f))]\n",
        "    random.shuffle(all_images)  # Shuffle to ensure randomness\n",
        "\n",
        "    total_images = len(all_images)\n",
        "    train_count = int(0.8 * total_images)\n",
        "    val_count = int(0.1 * total_images)\n",
        "    test_count = total_images - train_count - val_count  # To ensure all images are included\n",
        "\n",
        "    train_images = all_images[:train_count]\n",
        "    val_images = all_images[train_count:train_count + val_count]\n",
        "    test_images = all_images[train_count + val_count:]\n",
        "\n",
        "    def copy_images(image_list, source_folder, destination_folder):\n",
        "        for image in image_list:\n",
        "            src_path = os.path.join(source_folder, image)\n",
        "            dst_path = os.path.join(destination_folder, image)\n",
        "            shutil.copyfile(src_path, dst_path)\n",
        "\n",
        "    copy_images(train_images, source_folder, train_folder)\n",
        "    copy_images(val_images, source_folder, val_folder)\n",
        "    copy_images(test_images, source_folder, test_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "nMbwv4spDTp3",
        "outputId": "6d8e807e-9ee6-4a3f-e345-e33ff3985ed5"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-590dd76bcca2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msplit_and_copy_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_frames_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_real_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_real_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_real_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msplit_and_copy_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_frames_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_fake_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_fake_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_fake_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-40d9e91e9114>\u001b[0m in \u001b[0;36msplit_and_copy_images\u001b[0;34m(source_folder, train_folder, val_folder, test_folder)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mcopy_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mcopy_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mcopy_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-40d9e91e9114>\u001b[0m in \u001b[0;36mcopy_images\u001b[0;34m(image_list, source_folder, destination_folder)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0msrc_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mdst_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mcopy_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                     \u001b[0;31m# macOS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0m_HAS_FCOPYFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                             \u001b[0m_fastcopy_fcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_COPYFILE_DATA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "split_and_copy_images(real_frames_path, train_real_path, val_real_path, test_real_path)\n",
        "split_and_copy_images(fake_frames_path, train_fake_path, val_fake_path, test_fake_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBkEhfGhx974"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/Celeb-DF/Split-Frames'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'val')\n",
        "test_dir = os.path.join(base_dir, 'test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UWg4OOsDi3t",
        "outputId": "f515b54b-3eb3-4f49-c1db-71dd14131747"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 12571 images belonging to 2 classes.\n",
            "Found 1541 images belonging to 2 classes.\n",
            "Found 992 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Define ImageDataGenerator for data augmentation and normalization\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Create generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),  # Adjust based on your model input size\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "train_steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "val_steps_per_epoch = val_generator.samples // val_generator.batch_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CX4R2bdhp_jg"
      },
      "outputs": [],
      "source": [
        "# Assuming you have distillation and multimodal predictions from a previous model\n",
        "# Get predictions for training data\n",
        "# predictions_train = modelrgb.predict(X_train)\n",
        "# distillation_predictions_train = predictions_train[1]  # Access the second element of the list\n",
        "# multimodal_predictions_train = predictions_train[2]    # Access the third element\n",
        "\n",
        "# # Get predictions for testing data\n",
        "# predictions_test = modelrgb.predict(X_test)\n",
        "# distillation_predictions_test = predictions_test[1]\n",
        "# multimodal_predictions_test = predictions_test[2]\n",
        "\n",
        "def combined_generator(image_gen, distillation_data, multimodal_data):\n",
        "    while True:\n",
        "        # Get batch from image generator\n",
        "        image_data, image_labels = next(image_gen)\n",
        "\n",
        "        # Generate dummy data for MFCC and FFT (replace with actual data)\n",
        "        batch_size = image_data.shape[0]\n",
        "        mfcc_data = np.zeros((batch_size, 13, 2))  # Dummy MFCC data\n",
        "        fft_data = np.zeros((batch_size, 974))     # Dummy FFT data\n",
        "\n",
        "        # Convert labels to one-hot encoding\n",
        "        image_labels = tf.keras.utils.to_categorical(image_labels, num_classes=2)\n",
        "\n",
        "        # Select a batch of distillation and multimodal data\n",
        "        start_idx = np.random.randint(0, len(distillation_data) - batch_size)\n",
        "        batch_distillation_data = distillation_data[start_idx: start_idx + batch_size]\n",
        "        batch_multimodal_data = multimodal_data[start_idx: start_idx + batch_size]\n",
        "\n",
        "        yield [mfcc_data, fft_data, batch_distillation_data,image_data], image_labels\n",
        "\n",
        "# Combined training generator\n",
        "combined_train_gen = combined_generator(train_generator, distillation_predictions_train, multimodal_predictions_train)\n",
        "\n",
        "# Combined validation generator\n",
        "combined_val_gen = combined_generator(val_generator, distillation_predictions_test, multimodal_predictions_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2TR38fEzfb0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define batch size for generators\n",
        "batch_size = 16\n",
        "\n",
        "\n",
        "def combined_generator(real_gen, fake_gen, distillation_data, multimodal_data):\n",
        "    while True:\n",
        "        # Get batches from both generators simultaneously\n",
        "        real_data, real_labels = next(real_gen)\n",
        "        fake_data, fake_labels = next(fake_gen)\n",
        "\n",
        "        # Combine batches\n",
        "        combined_images = np.concatenate((real_data, fake_data), axis=0)\n",
        "        combined_labels = np.concatenate((real_labels, fake_labels), axis=0)\n",
        "\n",
        "        # Shuffle combined batch\n",
        "        combined_images, combined_labels = shuffle(combined_images, combined_labels, random_state=42)\n",
        "\n",
        "        # Generate dummy data for MFCC and FFT (replace with actual data)\n",
        "        batch_size = combined_images.shape[0]\n",
        "        mfcc_data = np.zeros((batch_size, 13, 2))  # Dummy MFCC data\n",
        "        fft_data = np.zeros((batch_size, 974))     # Dummy FFT data\n",
        "\n",
        "        # Convert labels to one-hot encoding\n",
        "        combined_labels = tf.keras.utils.to_categorical(combined_labels, num_classes=2)\n",
        "\n",
        "        # Select a batch of distillation and multimodal data\n",
        "        start_idx = np.random.randint(0, len(distillation_data) - batch_size)\n",
        "        batch_distillation_data = distillation_data[start_idx: start_idx + batch_size]\n",
        "        batch_multimodal_data = multimodal_data[start_idx: start_idx + batch_size]\n",
        "\n",
        "        yield [mfcc_data, fft_data, batch_distillation_data, batch_multimodal_data], combined_labels\n",
        "\n",
        "# Combined training generator (pass your actual data)\n",
        "combined_gen = combined_generator(train_generator, train_generator, distillation_predictions_train, multimodal_predictions_train)\n",
        "\n",
        "# Combined validation generator\n",
        "validation_gen = combined_generator(val_generator, val_generator, distillation_predictions_test, multimodal_predictions_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 859
        },
        "id": "RjKHiQ2Yp_gE",
        "outputId": "7a8e41bc-c237-44d4-cabf-5dfc7eaf2163"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MFCC shape: (32, 13, 2)\n",
            "FFT shape: (32, 974)\n",
            "Distillation shape: (32, 1)\n",
            "Image shape: (32, 150, 150, 3)\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 3580, in concatenate\n        return tf.concat([to_dense(x) for x in tensors], axis)\n\n    ValueError: Exception encountered when calling layer 'concatenate_7' (type Concatenate).\n    \n    Shape must be rank 2 but is rank 4 for '{{node model_5/concatenate_7/concat}} = ConcatV2[N=3, T=DT_FLOAT, Tidx=DT_INT32](IteratorGetNext:2, IteratorGetNext:3, model_5/flatten_18/Reshape, model_5/concatenate_7/concat/axis)' with input shapes: [?,?], [?,?,?,?], [?,1], [].\n    \n    Call arguments received by layer 'concatenate_7' (type Concatenate):\n       inputs=['tf.Tensor(shape=(None, None), dtype=float32)', 'tf.Tensor(shape=(None, None, None, None), dtype=float32)', 'tf.Tensor(shape=(None, 1), dtype=float32)']\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-884aba707fe4>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcombined_train_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_steps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Adjust based on your requirements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 3580, in concatenate\n        return tf.concat([to_dense(x) for x in tensors], axis)\n\n    ValueError: Exception encountered when calling layer 'concatenate_7' (type Concatenate).\n    \n    Shape must be rank 2 but is rank 4 for '{{node model_5/concatenate_7/concat}} = ConcatV2[N=3, T=DT_FLOAT, Tidx=DT_INT32](IteratorGetNext:2, IteratorGetNext:3, model_5/flatten_18/Reshape, model_5/concatenate_7/concat/axis)' with input shapes: [?,?], [?,?,?,?], [?,1], [].\n    \n    Call arguments received by layer 'concatenate_7' (type Concatenate):\n       inputs=['tf.Tensor(shape=(None, None), dtype=float32)', 'tf.Tensor(shape=(None, None, None, None), dtype=float32)', 'tf.Tensor(shape=(None, 1), dtype=float32)']\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(\n",
        "    combined_train_gen,\n",
        "    steps_per_epoch=train_steps_per_epoch,\n",
        "    epochs=3,  # Adjust based on your requirements\n",
        "    validation_data=combined_val_gen,\n",
        "    validation_steps=val_steps_per_epoch\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEWOQLhdp_eB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vREfVEXp_bu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "CCYcYT1LGtfI",
        "outputId": "aac115a7-e842-4e23-f4dd-bce4f91917f1"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Expected sequence or array-like, got <class 'keras.src.engine.keras_tensor.KerasTensor'>",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtype_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/keras_tensor.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0;34m\"Keras symbolic inputs/outputs do not \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Keras symbolic inputs/outputs do not implement `__len__`. You may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model. This error will also get raised if you try asserting a symbolic input/output directly.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-16a1c59c6111>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Assuming 'mfcc_data' contains all your MFCC data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmfcc_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmfcc_val_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_mfcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfft_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfft_val_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0minput_distillation_second_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_distillation_second_val_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_distillation_second\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2557\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one array required as input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2559\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \"\"\"\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \"\"\"\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtype_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtype_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Expected sequence or array-like, got <class 'keras.src.engine.keras_tensor.KerasTensor'>"
          ]
        }
      ],
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# mfcc_train_data, mfcc_val_data = train_test_split(input_mfcc, test_size=0.2, random_state=42)\n",
        "# fft_train_data, fft_val_data = train_test_split(input_fft, test_size=0.2, random_state=42)\n",
        "# input_distillation_second_train_data, input_distillation_second_val_data = train_test_split(input_distillation_second, test_size=0.2, random_state=42)\n",
        "# input_multimodal_second_train_data, input_multimodal_second_val_data = train_test_split(input_multimodal_second, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkH79XCIFIK2"
      },
      "outputs": [],
      "source": [
        "# # Create datasets for the additional inputs\n",
        "# train_mfcc_dataset = tf.data.Dataset.from_tensor_slices(mfcc_train_data).batch(train_generator.batch_size)\n",
        "# train_fft_dataset = tf.data.Dataset.from_tensor_slices(fft_train_data).batch(train_generator.batch_size)\n",
        "# train_distillation_dataset = tf.data.Dataset.from_tensor_slices(input_distillation_second_train_data).batch(train_generator.batch_size)\n",
        "# train_multimodal_dataset = tf.data.Dataset.from_tensor_slices(input_multimodal_second_train_data).batch(train_generator.batch_size)\n",
        "\n",
        "# val_mfcc_dataset = tf.data.Dataset.from_tensor_slices(mfcc_val_data).batch(val_generator.batch_size)\n",
        "# val_fft_dataset = tf.data.Dataset.from_tensor_slices(fft_val_data).batch(val_generator.batch_size)\n",
        "# val_distillation_dataset = tf.data.Dataset.from_tensor_slices(input_distillation_second_val_data).batch(val_generator.batch_size)  # Assuming you have validation data for these\n",
        "# val_multimodal_dataset = tf.data.Dataset.from_tensor_slices(input_multimodal_second_val_data).batch(val_generator.batch_size)    # Assuming you have validation data for these\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "MP1plmChDtiR",
        "outputId": "41ed2b85-840f-408c-ed0b-eb427bcc06de"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b178b85b9fc0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mfcc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_steps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Set based on the combined number of batches for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "# history = model.fit(\n",
        "#     train_generator, input_fft, input_mfcc,\n",
        "#     steps_per_epoch=train_steps_per_epoch,  # Set based on the combined number of batches for training\n",
        "#     epochs=2,\n",
        "#     validation_data=val_generator,\n",
        "#     validation_steps=val_steps_per_epoch  # Set based on the combined number of batches for validation\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALn8Rd6RDvUF"
      },
      "outputs": [],
      "source": [
        "# test_steps_per_epoch = test_generator.samples // test_generator.batch_size\n",
        "\n",
        "# test_loss, test_acc = model.evaluate(\n",
        "#     test_generator,\n",
        "#     steps=test_steps_per_epoch\n",
        "# )\n",
        "\n",
        "# print(f'Test accuracy: {test_acc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Chh5JSnOxTq2"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# import os\n",
        "# import shutil\n",
        "# import numpy as np\n",
        "# from sklearn.utils import shuffle\n",
        "\n",
        "# # Paths to your directories\n",
        "# real_frames_path = '/content/drive/MyDrive/Celeb-DF/Frames-Real'\n",
        "# fake_frames_path = '/content/drive/MyDrive/Celeb-DF/Frames-Fake'\n",
        "\n",
        "# def split_and_copy(src_dir, dst_dirs, split_ratios=(0.7, 0.15, 0.15)):\n",
        "#     # List all files in the source directory\n",
        "#     files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n",
        "\n",
        "#     # Shuffle files randomly\n",
        "#     np.random.shuffle(files)\n",
        "\n",
        "#     # Calculate number of files for each split\n",
        "#     num_files = len(files)\n",
        "#     num_train = int(split_ratios[0] * num_files)\n",
        "#     num_val = int(split_ratios[1] * num_files)\n",
        "#     num_test = num_files - num_train - num_val\n",
        "\n",
        "#     # Assign files to splits\n",
        "#     train_files = files[:num_train]\n",
        "#     val_files = files[num_train:num_train + num_val]\n",
        "#     test_files = files[num_train + num_val:]\n",
        "\n",
        "#     # Copy files to destination directories\n",
        "#     for file in train_files:\n",
        "#         src_path = os.path.join(src_dir, file)\n",
        "#         dst_path = os.path.join(dst_dirs['train'], file)\n",
        "#         try:\n",
        "#             shutil.copy(src_path, dst_path)\n",
        "#             print(f\"Copied {src_path} to {dst_path}\")  # Print successful copies\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error copying {src_path} to {dst_path}: {e}\")  # Print errors\n",
        "\n",
        "#     # Repeat the same process for validation and test files\n",
        "#     # ... (rest of the function remains the same)\n",
        "\n",
        "#     # Copy files to destination directories\n",
        "#     for file in train_files:\n",
        "#         shutil.copy(os.path.join(src_dir, file), os.path.join(dst_dirs['train'], file))\n",
        "#     for file in val_files:\n",
        "#         shutil.copy(os.path.join(src_dir, file), os.path.join(dst_dirs['val'], file))\n",
        "#     for file in test_files:\n",
        "#         shutil.copy(os.path.join(src_dir, file), os.path.join(dst_dirs['test'], file))\n",
        "\n",
        "# # Output paths for splits\n",
        "# output_base_path = '/content/split'\n",
        "# output_real_path = os.path.join(output_base_path, 'real')\n",
        "# output_fake_path = os.path.join(output_base_path, 'fake')\n",
        "\n",
        "# # Create directories for splits, ensuring they exist\n",
        "# os.makedirs(os.path.join(output_real_path, 'train'), exist_ok=True)\n",
        "# os.makedirs(os.path.join(output_real_path, 'val'), exist_ok=True)\n",
        "# os.makedirs(os.path.join(output_real_path, 'test'), exist_ok=True)\n",
        "\n",
        "# os.makedirs(os.path.join(output_fake_path, 'train'), exist_ok=True)\n",
        "# os.makedirs(os.path.join(output_fake_path, 'val'), exist_ok=True)\n",
        "# os.makedirs(os.path.join(output_fake_path, 'test'), exist_ok=True)\n",
        "\n",
        "# # ... (rest of your code)\n",
        "\n",
        "# # Split and copy real frames\n",
        "# split_and_copy(real_frames_path, {'train': os.path.join(output_real_path, 'train'),\n",
        "#                                   'val': os.path.join(output_real_path, 'val'),\n",
        "#                                   'test': os.path.join(output_real_path, 'test')})\n",
        "\n",
        "# # Split and copy fake frames\n",
        "# split_and_copy(fake_frames_path, {'train': os.path.join(output_fake_path, 'train'),\n",
        "#                                   'val': os.path.join(output_fake_path, 'val'),\n",
        "#                                   'test': os.path.join(output_fake_path, 'test')})\n",
        "\n",
        "# # Create an ImageDataGenerator instance\n",
        "# datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# # Define batch size for generators\n",
        "# batch_size = 16\n",
        "\n",
        "# # Training Generators\n",
        "# train_real_generator = datagen.flow_from_directory(\n",
        "#     directory=os.path.join(output_real_path, 'train'),\n",
        "#     target_size=(128, 128),\n",
        "#     batch_size=batch_size,\n",
        "#     class_mode='binary',\n",
        "#     shuffle=True\n",
        "# )\n",
        "\n",
        "# train_fake_generator = datagen.flow_from_directory(\n",
        "#     directory=os.path.join(output_fake_path, 'train'),\n",
        "#     target_size=(128, 128),\n",
        "#     batch_size=batch_size,\n",
        "#     class_mode='binary',\n",
        "#     shuffle=True\n",
        "# )\n",
        "\n",
        "# # Validation Generators\n",
        "# val_real_generator = datagen.flow_from_directory(\n",
        "#     directory=os.path.join(output_real_path, 'val'),\n",
        "#     target_size=(128, 128),\n",
        "#     batch_size=batch_size,\n",
        "#     class_mode='binary',\n",
        "#     shuffle=False\n",
        "# )\n",
        "\n",
        "# val_fake_generator = datagen.flow_from_directory(\n",
        "#     directory=os.path.join(output_fake_path, 'val'),\n",
        "#     target_size=(128, 128),\n",
        "#     batch_size=batch_size,\n",
        "#     class_mode='binary',\n",
        "#     shuffle=False\n",
        "# )\n",
        "\n",
        "# # Testing Generators\n",
        "# test_real_generator = datagen.flow_from_directory(\n",
        "#     directory=os.path.join(output_real_path, 'test'),\n",
        "#     target_size=(128, 128),\n",
        "#     batch_size=batch_size,\n",
        "#     class_mode='binary',\n",
        "#     shuffle=False\n",
        "# )\n",
        "\n",
        "# test_fake_generator = datagen.flow_from_directory(\n",
        "#     directory=os.path.join(output_fake_path, 'test'),\n",
        "#     target_size=(128, 128),\n",
        "#     batch_size=batch_size,\n",
        "#     class_mode='binary',\n",
        "#     shuffle=False\n",
        "# )\n",
        "\n",
        "# # Combined generator function\n",
        "# def combined_generator(real_gen, fake_gen, distillation_data, multimodal_data):\n",
        "#     while True:\n",
        "#         # Get batches from both generators simultaneously\n",
        "#         real_data, real_labels = next(real_gen)\n",
        "#         fake_data, fake_labels = next(fake_gen)\n",
        "\n",
        "#         # Combine batches\n",
        "#         combined_images = np.concatenate((real_data, fake_data), axis=0)\n",
        "#         combined_labels = np.concatenate((real_labels, fake_labels), axis=0)\n",
        "\n",
        "#         # Shuffle combined batch\n",
        "#         combined_images, combined_labels = shuffle(combined_images, combined_labels, random_state=42)\n",
        "\n",
        "#         # Generate dummy data for MFCC and FFT (replace with actual data)\n",
        "#         batch_size = combined_images.shape[0]\n",
        "#         mfcc_data = np.zeros((batch_size, 13, 2))  # Dummy MFCC data\n",
        "#         fft_data = np.zeros((batch_size, 974))     # Dummy FFT data\n",
        "\n",
        "#         # Ensure labels are within the valid range [0, num_classes-1]\n",
        "#         combined_labels = np.clip(combined_labels, 0, 1) # Clip labels to be within [0, 1]\n",
        "\n",
        "#         # Convert labels to one-hot encoding\n",
        "#         combined_labels = tf.keras.utils.to_categorical(combined_labels, num_classes=2)\n",
        "\n",
        "#         # Select a batch of distillation and multimodal data\n",
        "#         start_idx = np.random.randint(0, len(distillation_data) - batch_size)\n",
        "#         batch_distillation_data = distillation_data[start_idx: start_idx + batch_size]\n",
        "#         batch_multimodal_data = multimodal_data[start_idx: start_idx + batch_size]\n",
        "\n",
        "#         yield [mfcc_data, fft_data, batch_distillation_data, batch_multimodal_data], combined_labels\n",
        "\n",
        "# # Create combined generators\n",
        "# combined_gen = combined_generator(train_real_generator, train_fake_generator, distillation_predictions_train, multimodal_predictions_train)\n",
        "# combined_val_gen = combined_generator(val_real_generator, val_fake_generator, distillation_predictions_train, multimodal_predictions_train)\n",
        "# combined_test_gen = combined_generator(test_real_generator, test_fake_generator, distillation_predictions_test, multimodal_predictions_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIL-Mcdj96qV",
        "outputId": "d7e35c81-72f7-4bcd-bcf7-0e4f6d1f66d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Real Files: []\n",
            "Validation Real Files: []\n",
            "Validation Real Files: []\n",
            "Validation Real Files: []\n"
          ]
        }
      ],
      "source": [
        "print(\"Training Real Files:\", train_real_generator.filepaths[:1])  # Print first 5 file paths\n",
        "print(\"Validation Real Files:\", val_real_generator.filepaths[:1])\n",
        "print(\"Validation Real Files:\", val_fake_generator.filepaths[:3])\n",
        "print(\"Validation Real Files:\", test_real_generator.filepaths[:3])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guDoSk_3xYJL"
      },
      "outputs": [],
      "source": [
        "# Determine the number of steps per epoch based on the dataset size and batch size\n",
        "train_steps_per_epoch = (len(train_real_generator) + len(train_fake_generator)) // batch_size\n",
        "val_steps_per_epoch = (len(val_real_generator) + len(val_fake_generator)) // batch_size\n",
        "test_steps_per_epoch = (len(test_real_generator) + len(test_fake_generator)) // batch_size\n",
        "\n",
        "print(\"Train steps per epoch:\", train_steps_per_epoch)\n",
        "print(\"Validation steps per epoch:\", val_steps_per_epoch)\n",
        "print(\"Test steps per epoch:\", test_steps_per_epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Un9PcPia7MBC"
      },
      "outputs": [],
      "source": [
        "# Train the model using the updated generators\n",
        "history = model.fit(\n",
        "    combined_gen,\n",
        "    steps_per_epoch=train_steps_per_epoch,  # Set based on the combined number of batches for training\n",
        "    epochs=2,\n",
        "    validation_data=combined_val_gen,\n",
        "    validation_steps=val_steps_per_epoch  # Set based on the combined number of batches for validation\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlVozqONqOA5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Evaluate the model using the test generator\n",
        "test_loss, test_acc = model.evaluate(\n",
        "    combined_test_gen,\n",
        "    steps=test_steps_per_epoch  # Set based on the combined number of batches for testing\n",
        ")\n",
        "\n",
        "print(f\"Test accuracy: {test_acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zG7zHXYrqS6C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P21I86KHqNUm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87DO4LHh7kK2"
      },
      "outputs": [],
      "source": [
        "# # Define paths to your zip files\n",
        "# real_zip_path = '/content/drive/MyDrive/Celeb-DF/Frames-Real-20240626T130257Z-001.zip'\n",
        "# fake_zip_path = '/content/drive/MyDrive/Celeb-DF/Frames-Fake-20240626T130258Z-001.zip'\n",
        "\n",
        "# # Unzip the files\n",
        "# !unzip -q -o '{real_zip_path}' -d '/content/Frames-Real'\n",
        "# !unzip -q -o '{fake_zip_path}' -d '/content/Frames-Fake'\n",
        "\n",
        "# #  --- Data Generators ---\n",
        "# datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)  # Add validation split\n",
        "\n",
        "# # Define batch size for generators\n",
        "# batch_size = 16\n",
        "\n",
        "# # Training generator\n",
        "# real_generator = datagen.flow_from_directory(\n",
        "#     directory='/content/Frames-Real',\n",
        "#     target_size=(128, 128),\n",
        "#     batch_size=batch_size,\n",
        "#     class_mode='binary',\n",
        "#     shuffle=True,\n",
        "#     subset='training'  # Use training subset\n",
        "# )\n",
        "\n",
        "# fake_generator = datagen.flow_from_directory(\n",
        "#     directory='/content/Frames-Fake',\n",
        "#     target_size=(128, 128),\n",
        "#     batch_size=batch_size,\n",
        "#     class_mode='binary',\n",
        "#     shuffle=True,\n",
        "#     subset='training'  # Use training subset\n",
        "# )\n",
        "\n",
        "# # Validation generator\n",
        "# real_val_generator = datagen.flow_from_directory(\n",
        "#     directory='/content/Frames-Real',\n",
        "#     target_size=(128, 128),\n",
        "#     batch_size=batch_size,\n",
        "#     class_mode='binary',\n",
        "#     shuffle=False,  # No need to shuffle validation data\n",
        "#     subset='validation'  # Use validation subset\n",
        "# )\n",
        "\n",
        "# fake_val_generator = datagen.flow_from_directory(\n",
        "#     directory='/content/Frames-Fake',\n",
        "#     target_size=(128, 128),\n",
        "#     batch_size=batch_size,\n",
        "#     class_mode='binary',\n",
        "#     shuffle=False,  # No need to shuffle validation data\n",
        "#     subset='validation'  # Use validation subset\n",
        "# )\n",
        "\n",
        "# def combined_generator(real_gen, fake_gen, distillation_data, multimodal_data):\n",
        "#     while True:\n",
        "#         # Get batches from both generators simultaneously\n",
        "#         real_data, real_labels = next(real_gen)\n",
        "#         fake_data, fake_labels = next(fake_gen)\n",
        "\n",
        "#         # Combine batches\n",
        "#         combined_images = np.concatenate((real_data, fake_data), axis=0)\n",
        "#         combined_labels = np.concatenate((real_labels, fake_labels), axis=0)\n",
        "\n",
        "#         # Shuffle combined batch\n",
        "#         combined_images, combined_labels = shuffle(combined_images, combined_labels, random_state=42)\n",
        "\n",
        "#         # Generate dummy data for MFCC and FFT (replace with actual data)\n",
        "#         batch_size = combined_images.shape[0]\n",
        "#         mfcc_data = np.zeros((batch_size, 13, 2))  # Dummy MFCC data\n",
        "#         fft_data = np.zeros((batch_size, 974))     # Dummy FFT data\n",
        "\n",
        "#         # Convert labels to one-hot encoding\n",
        "#         combined_labels = tf.keras.utils.to_categorical(combined_labels, num_classes=2)\n",
        "\n",
        "#         # Select a batch of distillation and multimodal data\n",
        "#         start_idx = np.random.randint(0, len(distillation_data) - batch_size)\n",
        "#         batch_distillation_data = distillation_data[start_idx: start_idx + batch_size]\n",
        "#         batch_multimodal_data = multimodal_data[start_idx: start_idx + batch_size]\n",
        "\n",
        "#         yield [mfcc_data, fft_data, batch_distillation_data, batch_multimodal_data], combined_labels\n",
        "\n",
        "# # Combined training generator (pass your actual data)\n",
        "# combined_gen = combined_generator(real_generator, fake_generator, distillation_predictions_train, multimodal_predictions_train)\n",
        "\n",
        "# # Combined validation generator\n",
        "# validation_gen = combined_generator(real_val_generator, fake_val_generator, distillation_predictions_test, multimodal_predictions_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ahQawUZqDyZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5CMQ2aQdHWT"
      },
      "outputs": [],
      "source": [
        "# # Train the model using the generator\n",
        "# history = model.fit(\n",
        "#     combined_gen,\n",
        "#     steps_per_epoch=200,  # Adjust based on your dataset size\n",
        "#     epochs=5,\n",
        "#     validation_data=validation_gen,\n",
        "#     validation_steps=50  # Adjust based on your validation dataset size\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ek3BVHZR8Xh7"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "\n",
        "# # Directory where your test images are stored\n",
        "# test_data_dir = '/content/drive/MyDrive/Celeb-DF/Frames-YouTube-real'\n",
        "\n",
        "# # Get list of image files in the directory\n",
        "# image_files = [f for f in os.listdir(test_data_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "# # Initialize lists to store image data and dummy labels (not used for prediction)\n",
        "# test_images = []\n",
        "# dummy_labels = []  # Placeholder, won't be used\n",
        "\n",
        "# # Load and preprocess images\n",
        "# for image_file in image_files:\n",
        "#     image_path = os.path.join(test_data_dir, image_file)\n",
        "#     img = load_img(image_path, target_size=(128, 128))  # Load and resize image\n",
        "#     img_array = img_to_array(img) / 255.0  # Convert to array and rescale\n",
        "#     test_images.append(img_array)\n",
        "#     dummy_labels.append(0)  # Append a dummy label\n",
        "\n",
        "# # Convert lists to numpy arrays\n",
        "# test_images = np.array(test_images)\n",
        "# dummy_labels = np.array(dummy_labels)\n",
        "\n",
        "# # Generate dummy data for MFCC, FFT, distillation, and multimodal inputs\n",
        "# num_test_samples = test_images.shape[0]\n",
        "# mfcc_data = np.zeros((num_test_samples, 13, 2))  # Dummy MFCC data\n",
        "# fft_data = np.zeros((num_test_samples, 974))     # Dummy FFT data\n",
        "# distillation_data = np.zeros((num_test_samples, 1))  # Dummy distillation data\n",
        "# multimodal_data = np.zeros((num_test_samples, 1))    # Dummy multimodal data\n",
        "\n",
        "# # Make predictions\n",
        "# predictions = model.predict([mfcc_data, fft_data, distillation_data, multimodal_data])\n",
        "\n",
        "# # Interpret predictions (assuming binary classification)\n",
        "# predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# # Print or further process the predicted classes\n",
        "# print(predicted_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_Jtod6esLZy"
      },
      "source": [
        "complete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13aTJp_Lrn1I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S93NvWBGsG7y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwpSB-dNsG4d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-iZVzYBsG2R"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNc36tFEsGz6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Y0Y_qoAsGx2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tvt8220HsGvf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAxIU5ErsGtI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcyPhse6sGq-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnwIJ3N-JFv-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, LayerNormalization, Reshape, Add, MultiHeadAttention, Dropout, LayerNormalization, Conv1D\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "# Define input shapes for MFCC and FFT features\n",
        "input_mfcc_shape = (13, 2)  # MFCC shape: 13 coefficients, 2 time steps (assuming 2 channels)\n",
        "input_fft_shape = (974,)  # FFT shape: 974 features\n",
        "\n",
        "# Inputs for MFCC and FFT features\n",
        "input_mfcc = Input(shape=input_mfcc_shape, name='input_mfcc')\n",
        "input_fft = Input(shape=input_fft_shape, name='input_fft')\n",
        "\n",
        "# Patch Embedding layer for MFCC and FFT\n",
        "projection_dim = 256  # Adjust as needed\n",
        "patch_mfcc = Dense(projection_dim)(input_mfcc)\n",
        "patch_fft = Dense(projection_dim)(input_fft)\n",
        "\n",
        "# Reshape patch_fft to match the rank of patch_mfcc\n",
        "patch_fft = Reshape((-1, projection_dim))(patch_fft) # Reshape patch_fft to be 3D\n",
        "\n",
        "# Concatenate patches\n",
        "combined_patches = tf.concat([patch_mfcc, patch_fft], axis=1)\n",
        "\n",
        "# Reshape for PVT block\n",
        "combined_patches = Reshape((-1, projection_dim))(combined_patches)\n",
        "\n",
        "# Pyramid Vision Transformer (PVT) block\n",
        "def pvt_block(x, num_channels):\n",
        "    shortcut = x\n",
        "    # Projection layers\n",
        "    x = Dense(num_channels)(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = Dense(num_channels)(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    # Project the shortcut to match the number of channels\n",
        "    shortcut = Dense(num_channels * 4)(shortcut)\n",
        "    x = Dense(num_channels * 4)(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Add()([shortcut, x])\n",
        "    x = tf.nn.relu(x)\n",
        "    return x\n",
        "\n",
        "# Apply PVT block\n",
        "pvt_output = pvt_block(combined_patches, num_channels=256)  # Adjust num_channels as needed\n",
        "\n",
        "# Vision Transformer (ViT) block\n",
        "def vit_block(x, num_heads=8, ff_dim=512, dropout_rate=0.1):\n",
        "    # Multi-head self-attention\n",
        "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim)(x, x)\n",
        "    attn_output = Dropout(dropout_rate)(attn_output)\n",
        "    out1 = LayerNormalization(epsilon=1e-6)(attn_output + x)\n",
        "\n",
        "    # Feed Forward Network\n",
        "    ffn = Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(out1)\n",
        "    # Project back to the original dimension\n",
        "    ffn = Conv1D(filters=projection_dim, kernel_size=1)(ffn)\n",
        "    ffn = Dropout(dropout_rate)(ffn)\n",
        "\n",
        "    # Adjust the number of channels in the skip connection to match FFN output\n",
        "    out1 = Conv1D(filters=projection_dim, kernel_size=1)(out1)\n",
        "\n",
        "    out2 = LayerNormalization(epsilon=1e-6)(ffn + out1)\n",
        "    return out2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HSm5Y-IdK4o5"
      },
      "outputs": [],
      "source": [
        "# Apply ViT block\n",
        "vit_output = vit_block(pvt_output)  # Adjust parameters as needed\n",
        "\n",
        "# Vision Transformer head\n",
        "# Flatten the output to remove the temporal dimension\n",
        "vision_transformer_output = Dense(1, activation='sigmoid', name='vision_transformer_output')(tf.keras.layers.Flatten()(vit_output))  # Example for binary classification\n",
        "\n",
        "# Distillation head\n",
        "# Flatten the output from the PVT block before passing it to the Dense layer\n",
        "distillation_output = Dense(10, activation='softmax', name='distillation_output')(tf.keras.layers.Flatten()(pvt_output))  # Example with 10 classes for distillation\n",
        "# Create model\n",
        "model = Model(inputs=[input_mfcc, input_fft], outputs=[vision_transformer_output, distillation_output, patch_mfcc, patch_fft, pvt_output])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss={'vision_transformer_output': 'binary_crossentropy', 'distillation_output': 'categorical_crossentropy'},\n",
        "              metrics={'vision_transformer_output': 'accuracy', 'distillation_output': 'accuracy'})\n",
        "\n",
        "# Print model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kB7KcuhyODXj"
      },
      "outputs": [],
      "source": [
        "# Assuming X_mfcc and X_fft are already computed from the frames as shown earlier\n",
        "\n",
        "# Split the dataset into training and testing sets for frames, MFCC, and FFT\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split frames data\n",
        "X_frames_train, X_frames_test, _, _ = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split MFCC and FFT data\n",
        "X_mfcc_train, X_mfcc_test, y_mfcc_train, y_mfcc_test = train_test_split(X_mfcc, y, test_size=0.2, random_state=42)\n",
        "X_fft_train, X_fft_test, y_fft_train, y_fft_test = train_test_split(X_fft, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Assuming y_mfcc and y_fft are the same as y for the first pipeline\n",
        "\n",
        "print(f\"Frames Training data shape: {X_frames_train.shape}\")\n",
        "print(f\"Frames Testing data shape: {X_frames_test.shape}\")\n",
        "print(f\"MFCC Training data shape: {X_mfcc_train.shape}\")\n",
        "print(f\"MFCC Testing data shape: {X_mfcc_test.shape}\")\n",
        "print(f\"FFT Training data shape: {X_fft_train.shape}\")\n",
        "print(f\"FFT Testing data shape: {X_fft_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCbCJ3VEK_pA"
      },
      "outputs": [],
      "source": [
        "from keras.utils import to_categorical\n",
        "# Convert labels to categorical (assuming 10 classes for distillation)\n",
        "y_train_categories = to_categorical(y_train, num_classes=10)\n",
        "y_test_categories = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "\n",
        "# Define callbacks if needed (e.g., for early stopping, checkpointing)\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss', restore_best_weights=True),\n",
        "    # Add more callbacks as needed\n",
        "]\n",
        "\n",
        "# Compile model (if not compiled already)\n",
        "model.compile(optimizer='adam',\n",
        "              loss={'vision_transformer_output': 'binary_crossentropy', 'distillation_output': 'categorical_crossentropy'},\n",
        "              metrics={'vision_transformer_output': 'accuracy', 'distillation_output': 'accuracy'})\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    {'input_mfcc': X_mfcc_train, 'input_fft': X_fft_train},\n",
        "    {'vision_transformer_output': y_train, 'distillation_output': y_train_categories},\n",
        "    epochs=10,  # Adjust number of epochs as needed\n",
        "    batch_size=32,  # Adjust batch size based on your hardware capabilities\n",
        "    validation_data=(\n",
        "        {'input_mfcc': X_mfcc_test, 'input_fft': X_fft_test},\n",
        "        {'vision_transformer_output': y_test, 'distillation_output': y_test_categories}\n",
        "    ),\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPBA4xUjm0Kz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oC3oUjQGm0HP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYglfOB9m0FO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqSaFh7um0C2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IG2T47dwm0Aw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, LayerNormalization, Reshape, Add, MultiHeadAttention, Dropout, LayerNormalization, Conv1D\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "# Define input shapes for MFCC and FFT features\n",
        "input_mfcc_shape = (13, 2)  # MFCC shape: 13 coefficients, 2 time steps (assuming 2 channels)\n",
        "input_fft_shape = (974,)  # FFT shape: 974 features\n",
        "\n",
        "# Inputs for MFCC and FFT features\n",
        "input_mfcc = Input(shape=input_mfcc_shape, name='input_mfcc')\n",
        "input_fft = Input(shape=input_fft_shape, name='input_fft')\n",
        "\n",
        "# Patch Embedding layer for MFCC and FFT\n",
        "projection_dim = 256  # Adjust as needed\n",
        "patch_mfcc = Dense(projection_dim)(input_mfcc)\n",
        "patch_fft = Dense(projection_dim)(input_fft)\n",
        "\n",
        "# Reshape patch_fft to match the rank of patch_mfcc\n",
        "patch_fft = Reshape((-1, projection_dim))(patch_fft) # Reshape patch_fft to be 3D\n",
        "\n",
        "# Concatenate patches\n",
        "combined_patches = tf.concat([patch_mfcc, patch_fft], axis=1)\n",
        "\n",
        "# Reshape for PVT block\n",
        "combined_patches = Reshape((-1, projection_dim))(combined_patches)\n",
        "\n",
        "# Pyramid Vision Transformer (PVT) block\n",
        "def pvt_block(x, num_channels):\n",
        "    shortcut = x\n",
        "    # Projection layers\n",
        "    x = Dense(num_channels)(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = Dense(num_channels)(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    # Project the shortcut to match the number of channels\n",
        "    shortcut = Dense(num_channels * 4)(shortcut)\n",
        "    x = Dense(num_channels * 4)(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Add()([shortcut, x])\n",
        "    x = tf.nn.relu(x)\n",
        "    return x\n",
        "\n",
        "# Apply PVT block\n",
        "pvt_output = pvt_block(combined_patches, num_channels=256)  # Adjust num_channels as needed\n",
        "\n",
        "# Vision Transformer (ViT) block\n",
        "def vit_block(x, num_heads=8, ff_dim=512, dropout_rate=0.1):\n",
        "    # Multi-head self-attention\n",
        "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim)(x, x)\n",
        "    attn_output = Dropout(dropout_rate)(attn_output)\n",
        "    out1 = LayerNormalization(epsilon=1e-6)(attn_output + x)\n",
        "\n",
        "    # Feed Forward Network\n",
        "    ffn = Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(out1)\n",
        "    # Project back to the original dimension\n",
        "    ffn = Conv1D(filters=projection_dim, kernel_size=1)(ffn)\n",
        "    ffn = Dropout(dropout_rate)(ffn)\n",
        "\n",
        "    # Adjust the number of channels in the skip connection to match FFN output\n",
        "    out1 = Conv1D(filters=projection_dim, kernel_size=1)(out1)\n",
        "\n",
        "    out2 = LayerNormalization(epsilon=1e-6)(ffn + out1)\n",
        "    return out2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jHADc7w8mz-p"
      },
      "outputs": [],
      "source": [
        "vit_output = vit_block(pvt_output)\n",
        "vision_transformer_output = Dense(1, activation='sigmoid', name='vision_transformer_output')(tf.keras.layers.Flatten()(vit_output))\n",
        "\n",
        "input_distillation_second = Input(shape=(1,), name='input_distillation_second')\n",
        "input_multimodal_second = Input(shape=(1,), name='input_multimodal_second')\n",
        "\n",
        "# Concatenate outputs (use the new input layer names)\n",
        "merged_output = Concatenate()([input_distillation_second, input_multimodal_second, vision_transformer_output])\n",
        "\n",
        "# Final classification head\n",
        "final_output = Dense(1, activation='sigmoid', name='output')(merged_output)\n",
        "\n",
        "distillation_predictions_train = tf.convert_to_tensor(distillation_predictions_train)\n",
        "multimodal_predictions_train = tf.convert_to_tensor(multimodal_predictions_train)\n",
        "\n",
        "#create model\n",
        "model = Model(inputs=[input_mfcc, input_fft, input_distillation_second, input_multimodal_second],\n",
        "              outputs=final_output)\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',  # Example loss function, adjust as needed\n",
        "              metrics=['accuracy'])         # Example metrics, adjust as needed\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuTQ2Zfxmz8q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define paths to your zip files\n",
        "real_zip_path = '/content/drive/MyDrive/Celeb-DF/Frames-Real-20240626T130257Z-001.zip'\n",
        "fake_zip_path = '/content/drive/MyDrive/Celeb-DF/Frames-Fake-20240626T130258Z-001.zip'\n",
        "\n",
        "# Unzip the files\n",
        "!unzip -q -o '{real_zip_path}' -d '/content/Frames-Real'\n",
        "!unzip -q -o '{fake_zip_path}' -d '/content/Frames-Fake'\n",
        "\n",
        "#  --- Data Generators ---\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)  # Add validation split\n",
        "\n",
        "# Define batch size for generators\n",
        "batch_size = 16\n",
        "\n",
        "# Training generator\n",
        "real_generator = datagen.flow_from_directory(\n",
        "    directory='/content/Frames-Real',\n",
        "    target_size=(128, 128),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle=True,\n",
        "    subset='training'  # Use training subset\n",
        ")\n",
        "\n",
        "fake_generator = datagen.flow_from_directory(\n",
        "    directory='/content/Frames-Fake',\n",
        "    target_size=(128, 128),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle=True,\n",
        "    subset='training'  # Use training subset\n",
        ")\n",
        "\n",
        "# Validation generator\n",
        "real_val_generator = datagen.flow_from_directory(\n",
        "    directory='/content/Frames-Real',\n",
        "    target_size=(128, 128),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle=False,  # No need to shuffle validation data\n",
        "    subset='validation'  # Use validation subset\n",
        ")\n",
        "\n",
        "fake_val_generator = datagen.flow_from_directory(\n",
        "    directory='/content/Frames-Fake',\n",
        "    target_size=(128, 128),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle=False,  # No need to shuffle validation data\n",
        "    subset='validation'  # Use validation subset\n",
        ")\n",
        "\n",
        "\n",
        "def combined_generator(real_gen, fake_gen):\n",
        "    while True:\n",
        "        # Get batches from both generators simultaneously\n",
        "        real_data, real_labels = next(real_gen)\n",
        "        fake_data, fake_labels = next(fake_gen)\n",
        "\n",
        "        # Combine batches\n",
        "        combined_images = np.concatenate((real_data, fake_data), axis=0)\n",
        "        combined_labels = np.concatenate((real_labels, fake_labels), axis=0)\n",
        "\n",
        "        # Shuffle combined batch\n",
        "        combined_images, combined_labels = shuffle(combined_images, combined_labels, random_state=42)\n",
        "\n",
        "        # Generate dummy data for MFCC and FFT (replace with actual data)\n",
        "        batch_size = combined_images.shape[0]\n",
        "        mfcc_data = np.zeros((batch_size, 13, 130))  # Dummy MFCC data\n",
        "        fft_data = np.zeros((batch_size, 130))       # Dummy FFT data\n",
        "\n",
        "        # Convert labels to one-hot encoding\n",
        "        combined_labels = tf.keras.utils.to_categorical(combined_labels, num_classes=2)\n",
        "\n",
        "        yield [mfcc_data, fft_data, combined_images], combined_labels\n",
        "\n",
        "# Combined training generator\n",
        "combined_gen = combined_generator(real_generator, fake_generator)\n",
        "\n",
        "# Combined validation generator\n",
        "validation_gen = combined_generator(real_val_generator, fake_val_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAPSUQymoEHq"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming X_mfcc, X_fft, and y are defined\n",
        "X_mfcc_train, X_mfcc_test, y_train, y_test = train_test_split(X_mfcc, y, test_size=0.2, random_state=42)\n",
        "X_fft_train, X_fft_test, _, _ = train_test_split(X_fft, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Distillation and multimodal outputs should not be split; they come directly from the first pipeline\n",
        "distillation_train = np.array(distillation_predictions_train)  # Ensure they are numpy arrays\n",
        "multimodal_train = np.array(multimodal_predictions_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvixcnBJoEE4"
      },
      "outputs": [],
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "# Convert labels to categorical (binary classification in this case)\n",
        "y_train_categories = to_categorical(y_train, num_classes=2)\n",
        "y_test_categories = to_categorical(y_test, num_classes=2)\n",
        "\n",
        "# One-hot encode distillation and multimodal outputs (if they are not already)\n",
        "distillation_train = to_categorical(distillation_predictions_train, num_classes=2)\n",
        "multimodal_train = to_categorical(multimodal_predictions_train, num_classes=2)\n",
        "\n",
        "\n",
        "# Squeeze the distillation and multimodal outputs to remove the extra dimension from one-hot encoding\n",
        "distillation_train = np.argmax(distillation_train, axis=1)\n",
        "multimodal_train = np.argmax(multimodal_train, axis=1)\n",
        "\n",
        "# Ensure that distillation_train and multimodal_train have the same length as your training data\n",
        "distillation_train = distillation_train[:len(X_mfcc_train)]\n",
        "multimodal_train = multimodal_train[:len(X_mfcc_train)]\n",
        "\n",
        "# Squeeze the distillation and multimodal outputs to remove the extra dimension from one-hot encoding\n",
        "merged_output = Concatenate()([input_distillation_second, input_multimodal_second, vision_transformer_output])\n",
        "\n",
        "# Final classification head (keep 2 output neurons for binary classification)\n",
        "final_output = Dense(2, activation='softmax', name='output')(merged_output)\n",
        "\n",
        "#create model (UPDATED)\n",
        "model = Model(inputs=[input_mfcc, input_fft, input_distillation_second, input_multimodal_second],\n",
        "              outputs=final_output)\n",
        "\n",
        "# Compile the model with the categorical_crossentropy loss function\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Make sure that X_mfcc_test, X_fft_test have the same number of samples as\n",
        "# distillation_train and multimodal_train in validation data\n",
        "X_mfcc_test = X_mfcc_test[:len(distillation_train)]\n",
        "X_fft_test = X_fft_test[:len(distillation_train)]\n",
        "y_test_categories = y_test_categories[:len(distillation_train)]\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    {'input_mfcc': X_mfcc_train, 'input_fft': X_fft_train,\n",
        "     'input_distillation_second': distillation_train, 'input_multimodal_second': multimodal_train},\n",
        "    y_train_categories,\n",
        "    epochs=5,\n",
        "    batch_size=16,\n",
        "    validation_data=(\n",
        "        {'input_mfcc': X_mfcc_test, 'input_fft': X_fft_test,\n",
        "         'input_distillation_second': distillation_train, 'input_multimodal_second': multimodal_train},\n",
        "        y_test_categories\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz5caMc-7Z1R"
      },
      "source": [
        "DUMMY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vx0wWWZo5Sey"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, LayerNormalization, Reshape, Add, MultiHeadAttention, Dropout, Concatenate, Conv1D\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Define input shapes for MFCC and FFT features\n",
        "input_mfcc_shape = (13, 2)\n",
        "input_fft_shape = (974,)\n",
        "\n",
        "# Inputs for MFCC and FFT features\n",
        "input_mfcc = Input(shape=input_mfcc_shape, name='input_mfcc')\n",
        "input_fft = Input(shape=input_fft_shape, name='input_fft')\n",
        "\n",
        "# Patch Embedding layer for MFCC and FFT\n",
        "projection_dim = 256\n",
        "patch_mfcc = Dense(projection_dim)(input_mfcc)\n",
        "patch_fft = Dense(projection_dim)(input_fft)\n",
        "\n",
        "# Reshape patch_fft to match the rank of patch_mfcc\n",
        "patch_fft = Reshape((-1, projection_dim))(patch_fft)\n",
        "\n",
        "# Concatenate patches\n",
        "combined_patches = tf.concat([patch_mfcc, patch_fft], axis=1)\n",
        "\n",
        "# Pyramid Vision Transformer (PVT) block\n",
        "def pvt_block(x, num_channels):\n",
        "    shortcut = x\n",
        "    x = Dense(num_channels)(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = Dense(num_channels)(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    shortcut = Dense(num_channels * 4)(shortcut)\n",
        "    x = Dense(num_channels * 4)(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Add()([shortcut, x])\n",
        "    x = tf.nn.relu(x)\n",
        "    return x\n",
        "\n",
        "# Apply PVT block\n",
        "pvt_output = pvt_block(combined_patches, num_channels=256)\n",
        "\n",
        "# Vision Transformer (ViT) block\n",
        "def vit_block(x, num_heads=8, ff_dim=512, dropout_rate=0.1):\n",
        "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim)(x, x)\n",
        "    attn_output = Dropout(dropout_rate)(attn_output)\n",
        "    out1 = LayerNormalization(epsilon=1e-6)(attn_output + x)\n",
        "    ffn = Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(out1)\n",
        "    ffn = Conv1D(filters=projection_dim, kernel_size=1)(ffn)\n",
        "    ffn = Dropout(dropout_rate)(ffn)\n",
        "    out1 = Conv1D(filters=projection_dim, kernel_size=1)(out1)\n",
        "    out2 = LayerNormalization(epsilon=1e-6)(ffn + out1)\n",
        "    return out2\n",
        "\n",
        "vit_output = vit_block(pvt_output)\n",
        "vision_transformer_output = Dense(1, activation='sigmoid', name='vision_transformer_output')(tf.keras.layers.Flatten()(vit_output))\n",
        "\n",
        "input_distillation_second = Input(shape=(1,), name='input_distillation_second')\n",
        "input_multimodal_second = Input(shape=(1,), name='input_multimodal_second')\n",
        "\n",
        "merged_output = Concatenate()([input_distillation_second, input_multimodal_second, vision_transformer_output])\n",
        "\n",
        "# Final classification head\n",
        "final_output = Dense(2, activation='softmax', name='output')(merged_output)\n",
        "\n",
        "# Create model\n",
        "model = Model(inputs=[input_mfcc, input_fft, input_distillation_second, input_multimodal_second], outputs=final_output)\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Generate dummy data for MFCC and FFT\n",
        "X_mfcc = np.random.rand(15426, 13, 2)\n",
        "X_fft = np.random.rand(15426, 974)\n",
        "y = np.random.randint(2, size=15426)\n",
        "distillation_predictions_train = np.random.randint(2, size=15426)\n",
        "multimodal_predictions_train = np.random.randint(2, size=15426)\n",
        "\n",
        "# Split data\n",
        "X_mfcc_train, X_mfcc_test, y_train, y_test = train_test_split(X_mfcc, y, test_size=0.2, random_state=42)\n",
        "X_fft_train, X_fft_test, _, _ = train_test_split(X_fft, y, test_size=0.2, random_state=42)\n",
        "distillation_train = distillation_predictions_train[:len(X_mfcc_train)]\n",
        "multimodal_train = multimodal_predictions_train[:len(X_mfcc_train)]\n",
        "\n",
        "# Convert labels to categorical\n",
        "y_train_categories = to_categorical(y_train, num_classes=2)\n",
        "y_test_categories = to_categorical(y_test, num_classes=2)\n",
        "\n",
        "# Convert distillation and multimodal outputs to categorical\n",
        "distillation_train = to_categorical(distillation_train, num_classes=2)\n",
        "multimodal_train = to_categorical(multimodal_train, num_classes=2)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    {'input_mfcc': X_mfcc_train, 'input_fft': X_fft_train,\n",
        "     'input_distillation_second': distillation_train[:, 1], 'input_multimodal_second': multimodal_train[:, 1]},\n",
        "    y_train_categories,\n",
        "    epochs=5,\n",
        "    batch_size=16,\n",
        "    validation_data=(\n",
        "        {'input_mfcc': X_mfcc_test, 'input_fft': X_fft_test,\n",
        "         'input_distillation_second': distillation_train[:len(X_mfcc_test), 1], 'input_multimodal_second': multimodal_train[:len(X_mfcc_test), 1]},\n",
        "        y_test_categories\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIpA48NVoEDD"
      },
      "outputs": [],
      "source": [
        "def combined_generator(real_gen, fake_gen, X_mfcc, X_fft, distillation, multimodal, y):\n",
        "    real_index = 0\n",
        "    fake_index = 0\n",
        "\n",
        "    while True:\n",
        "        # Get batches from both generators simultaneously\n",
        "        real_data, _ = next(real_gen)\n",
        "        fake_data, _ = next(fake_gen)\n",
        "\n",
        "        # Combine batches\n",
        "        combined_images = np.concatenate((real_data, fake_data), axis=0)\n",
        "        batch_size = combined_images.shape[0]\n",
        "\n",
        "        # Calculate the number of samples to take from each half\n",
        "        num_real = real_data.shape[0]\n",
        "        num_fake = fake_data.shape[0]\n",
        "\n",
        "\n",
        "        # Extract corresponding MFCC, FFT, distillation, multimodal, and labels\n",
        "        # Ensure consistent slicing for all data types\n",
        "        mfcc_data = np.concatenate((\n",
        "            X_mfcc[real_index : real_index + num_real],\n",
        "            X_mfcc[len(X_mfcc)//2 + fake_index : len(X_mfcc)//2 + fake_index + num_fake]\n",
        "        ))\n",
        "        fft_data = np.concatenate((\n",
        "            X_fft[real_index : real_index + num_real],\n",
        "            X_fft[len(X_fft)//2 + fake_index : len(X_fft)//2 + fake_index + num_fake]\n",
        "        ))\n",
        "        distillation_data = np.concatenate((\n",
        "            distillation[real_index : real_index + num_real],\n",
        "            distillation[len(distillation)//2 + fake_index : len(distillation)//2 + fake_index + num_fake]\n",
        "        ))\n",
        "        multimodal_data = np.concatenate((\n",
        "            multimodal[real_index : real_index + num_real],\n",
        "            multimodal[len(multimodal)//2 + fake_index : len(multimodal)//2 + fake_index + num_fake]\n",
        "        ))\n",
        "        combined_labels = np.concatenate((\n",
        "            y[real_index : real_index + num_real],\n",
        "            y[len(y)//2 + fake_index : len(y)//2 + fake_index + num_fake]\n",
        "        ))\n",
        "\n",
        "        # Update indices and reset if necessary (NEW CODE)\n",
        "        real_index = (real_index + num_real) % len(X_mfcc)\n",
        "        fake_index = (fake_index + num_fake) % len(X_mfcc)\n",
        "\n",
        "        print(\"Index update logic:\")\n",
        "        print(\"  real_index =\", real_index, \"(calculated as (\", real_index - num_real, \"+\", num_real, \") %\", len(X_mfcc), \")\")\n",
        "        print(\"  fake_index =\", fake_index, \"(calculated as (\", fake_index - num_fake, \"+\", num_fake, \") %\", len(X_mfcc), \")\")\n",
        "\n",
        "        # Shuffle indices instead of arrays directly\n",
        "        indices = np.arange(batch_size)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        # Apply shuffling to all arrays using the shuffled indices\n",
        "        combined_images = combined_images[indices]\n",
        "        mfcc_data = mfcc_data[indices]\n",
        "        fft_data = fft_data[indices]\n",
        "        distillation_data = distillation_data[indices]\n",
        "        multimodal_data = multimodal_data[indices]\n",
        "        combined_labels = combined_labels[indices]\n",
        "\n",
        "        print(\"Real index:\", real_index)\n",
        "        print(\"Fake index:\", fake_index)\n",
        "        print(\"Num real:\", num_real)\n",
        "        print(\"Num fake:\", num_fake)\n",
        "        print(\"Shapes:\", [arr.shape for arr in [mfcc_data, fft_data, distillation_data, multimodal_data, combined_labels]])\n",
        "\n",
        "        yield [mfcc_data, fft_data, distillation_data, multimodal_data], combined_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lOoHBls11B7"
      },
      "outputs": [],
      "source": [
        "print(\"X_mfcc_train shape:\", X_mfcc_train.shape)\n",
        "print(\"X_fft_train shape:\", X_fft_train.shape)\n",
        "print(\"distillation_train shape:\", distillation_train.shape)\n",
        "print(\"multimodal_train shape:\", multimodal_train.shape)\n",
        "print(\"y_train_categories shape:\", y_train_categories.shape)\n",
        "print(\"y shape:\", y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQpbfB_53SBj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ozj-uoRgyMbn"
      },
      "outputs": [],
      "source": [
        "# Create combined generators (pass precomputed data)\n",
        "combined_gen = combined_generator(\n",
        "    real_generator, fake_generator, X_mfcc_train, X_fft_train,\n",
        "    distillation_train, multimodal_train, y_train_categories\n",
        ")\n",
        "\n",
        "validation_gen = combined_generator(\n",
        "    real_val_generator, fake_val_generator, X_mfcc_test, X_fft_test,\n",
        "    distillation_train, multimodal_train, y_test_categories  # Use appropriate validation data\n",
        ")\n",
        "\n",
        "# Iterate over the generator for a few steps to see the print output\n",
        "for _ in range(3):  # Adjust the number of steps as needed\n",
        "    data, labels = next(combined_gen)\n",
        "    print(\"Data shapes:\", [arr.shape for arr in data], \"Labels shape:\", labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMjktd6roEA6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Train the model using the combined generators\n",
        "history = model.fit(\n",
        "    combined_gen,\n",
        "    epochs=10,\n",
        "    steps_per_epoch=len(real_generator),\n",
        "    validation_data=validation_gen,\n",
        "    validation_steps=len(real_val_generator)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ii33iuY-oD-v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKvg510toD9L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iFC-tCWoD7c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7P0RnjuoD5z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wv2_4BjoD23"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuQ1repimz6K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MW2y0RgtTA-g"
      },
      "outputs": [],
      "source": [
        "# Distillation and Multimodal Outputs from First Pipeline - Assuming placeholders\n",
        "# Replace with actual outputs from your first pipeline\n",
        "distillation_output_from_pipeline1 = Input(shape=(1,), name='distillation_output')\n",
        "multimodal_output_from_pipeline1 = Input(shape=(1,), name='multimodal_output')\n",
        "\n",
        "# Recreate necessary layers for the new model\n",
        "input_mfcc_new = Input(shape=input_mfcc_shape, name='input_mfcc_new')\n",
        "input_fft_new = Input(shape=input_fft_shape, name='input_fft_new')\n",
        "\n",
        "projection_dim = 256\n",
        "patch_mfcc_new = Dense(projection_dim)(input_mfcc_new)\n",
        "patch_fft_new = Dense(projection_dim)(input_fft_new)\n",
        "patch_fft_new = Reshape((-1, projection_dim))(patch_fft_new)\n",
        "combined_patches_new = tf.concat([patch_mfcc_new, patch_fft_new], axis=1)\n",
        "combined_patches_new = Reshape((-1, projection_dim))(combined_patches_new)\n",
        "pvt_output_new = pvt_block(combined_patches_new, num_channels=256)\n",
        "vit_output_new = vit_block(pvt_output_new)\n",
        "vision_transformer_output_new = Dense(1, activation='sigmoid', name='vision_transformer_output')(tf.keras.layers.Flatten()(vit_output_new))\n",
        "\n",
        "# Concatenate outputs from first pipeline and vision transformer output\n",
        "merged_output = Concatenate()([distillation_output_from_pipeline1, multimodal_output_from_pipeline1, vision_transformer_output_new])\n",
        "\n",
        "# Final classification head\n",
        "final_output = Dense(1, activation='sigmoid', name='output')(merged_output)\n",
        "\n",
        "# Create model\n",
        "model = Model(inputs=[input_mfcc_new, input_fft_new, distillation_output_from_pipeline1, multimodal_output_from_pipeline1],\n",
        "              outputs=final_output)\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',  # Example loss function, adjust as needed\n",
        "              metrics=['accuracy'])         # Example metrics, adjust as needed\n",
        "\n",
        "# Display model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqUW4z8hjmwh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define paths to your zip files\n",
        "real_zip_path = '/content/drive/MyDrive/Celeb-DF/Frames-Real-20240626T130257Z-001.zip'\n",
        "fake_zip_path = '/content/drive/MyDrive/Celeb-DF/Frames-Fake-20240626T130258Z-001.zip'\n",
        "\n",
        "# Unzip the files\n",
        "!unzip -q -o '{real_zip_path}' -d '/content/Frames-Real'\n",
        "!unzip -q -o '{fake_zip_path}' -d '/content/Frames-Fake'\n",
        "\n",
        "#  --- Data Generators ---\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)  # Add validation split\n",
        "\n",
        "# Define batch size for generators\n",
        "batch_size = 16\n",
        "\n",
        "# Training generator\n",
        "real_generator = datagen.flow_from_directory(\n",
        "    directory='/content/Frames-Real',\n",
        "    target_size=(128, 128),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle=True,\n",
        "    subset='training'  # Use training subset\n",
        ")\n",
        "\n",
        "fake_generator = datagen.flow_from_directory(\n",
        "    directory='/content/Frames-Fake',\n",
        "    target_size=(128, 128),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle=True,\n",
        "    subset='training'  # Use training subset\n",
        ")\n",
        "\n",
        "# Validation generator\n",
        "real_val_generator = datagen.flow_from_directory(\n",
        "    directory='/content/Frames-Real',\n",
        "    target_size=(128, 128),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle=False,  # No need to shuffle validation data\n",
        "    subset='validation'  # Use validation subset\n",
        ")\n",
        "\n",
        "fake_val_generator = datagen.flow_from_directory(\n",
        "    directory='/content/Frames-Fake',\n",
        "    target_size=(128, 128),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle=False,  # No need to shuffle validation data\n",
        "    subset='validation'  # Use validation subset\n",
        ")\n",
        "\n",
        "\n",
        "def combined_generator(real_gen, fake_gen):\n",
        "    while True:\n",
        "        # Get batches from both generators simultaneously\n",
        "        real_data, real_labels = next(real_gen)\n",
        "        fake_data, fake_labels = next(fake_gen)\n",
        "\n",
        "        # Combine batches\n",
        "        combined_images = np.concatenate((real_data, fake_data), axis=0)\n",
        "        combined_labels = np.concatenate((real_labels, fake_labels), axis=0)\n",
        "\n",
        "        # Shuffle combined batch\n",
        "        combined_images, combined_labels = shuffle(combined_images, combined_labels, random_state=42)\n",
        "\n",
        "        # Generate dummy data for MFCC and FFT (replace with actual data)\n",
        "        batch_size = combined_images.shape[0]\n",
        "        mfcc_data = np.zeros((batch_size, 13, 130))  # Dummy MFCC data\n",
        "        fft_data = np.zeros((batch_size, 130))       # Dummy FFT data\n",
        "\n",
        "        # Convert labels to one-hot encoding\n",
        "        combined_labels = tf.keras.utils.to_categorical(combined_labels, num_classes=2)\n",
        "\n",
        "        yield [mfcc_data, fft_data, combined_images], combined_labels\n",
        "\n",
        "# Combined training generator\n",
        "combined_gen = combined_generator(real_generator, fake_generator)\n",
        "\n",
        "# Combined validation generator\n",
        "validation_gen = combined_generator(real_val_generator, fake_val_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23VQuTkRAV6r"
      },
      "outputs": [],
      "source": [
        "#Get a sample batch from the generator to determine input shape\n",
        "sample_batch1 = next(combined_gen)\n",
        "# Print shapes of individual elements within sample_batch1[0] for debugging\n",
        "print([np.array(data).shape for data in sample_batch1[0]])\n",
        "\n",
        "# Assuming you want the shape of the image data (third element), proceed as follows:\n",
        "input_shape_gen = np.array(sample_batch1[0][2]).shape[1:] # Select the third element (index 2), convert to NumPy array, and remove batch dimension\n",
        "\n",
        "#Get a sample batch from the generator to determine input shape\n",
        "sample_batch2 = next(validation_gen)\n",
        "# Print shapes of individual elements within sample_batch1[0] for debugging\n",
        "print([np.array(data).shape for data in sample_batch2[0]])\n",
        "\n",
        "# Assuming you want the shape of the image data (third element), proceed as follows:\n",
        "input_shape_gen = np.array(sample_batch2[0][2]).shape[1:] # Select the third element (index 2), convert to NumPy array, and remove batch dimension\n",
        "\n",
        "\n",
        "# Now define input layers with the correct shapes\n",
        "input_mfcc = Input(shape=input_mfcc_new.shape[1:], name='input_mfcc')  # Use shape from previous input layer\n",
        "input_fft = Input(shape=input_fft_new.shape[1:], name='input_fft')    # Use shape from previous input layer\n",
        "input_gen = Input(shape=input_shape_gen[1:], name='input_gen')            # Use shape from generator sample, removing batch dimension\n",
        "\n",
        "input_combined = Input(shape=merged_output.shape[1:], name='input_combined') # Use shape from previous layer, removing batch dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXNjXnE3G9Ig"
      },
      "outputs": [],
      "source": [
        "# Assuming 'pvt_block' and 'vit_block' are defined elsewhere and accessible\n",
        "\n",
        "# Use the NEW input layers you defined for this model\n",
        "projection_dim = 256\n",
        "# Use input_mfcc, NOT input_mfcc_new\n",
        "patch_mfcc_new = Dense(projection_dim)(input_mfcc)\n",
        "# Use input_fft, NOT input_fft_new\n",
        "patch_fft_new = Dense(projection_dim)(input_fft)\n",
        "patch_fft_new = Reshape((-1, projection_dim))(patch_fft_new)\n",
        "combined_patches_new = tf.concat([patch_mfcc_new, patch_fft_new], axis=1)\n",
        "combined_patches_new = Reshape((-1, projection_dim))(combined_patches_new)\n",
        "pvt_output_new = pvt_block(combined_patches_new, num_channels=256)\n",
        "vit_output_new = vit_block(pvt_output_new)\n",
        "vision_transformer_output_new = Dense(1, activation='sigmoid', name='vision_transformer_output')(tf.keras.layers.Flatten()(vit_output_new))\n",
        "\n",
        "# Concatenate outputs from first pipeline and vision transformer output\n",
        "# Use the placeholder tensors for outputs from the first pipeline\n",
        "merged_output = Concatenate()([distillation_output_from_pipeline1, multimodal_output_from_pipeline1, vision_transformer_output_new])\n",
        "\n",
        "# Final classification head\n",
        "final_output = Dense(1, activation='sigmoid', name='output')(merged_output)\n",
        "\n",
        "# Create model with the specified inputs\n",
        "# Use the new input layers you defined\n",
        "model = Model(inputs=[input_mfcc, input_fft, input_gen,\n",
        "                      distillation_output_from_pipeline1, multimodal_output_from_pipeline1],\n",
        "              outputs=final_output)\n",
        "\n",
        "# Compile model (adjust loss and metrics as needed)\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYjPFWYNmjFP"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    combined_gen, # Pass the generator directly\n",
        "    epochs=10,\n",
        "    steps_per_epoch=len(real_generator),\n",
        "    validation_data=validation_gen,\n",
        "    validation_steps=len(real_val_generator),\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_x-0WU0i7kg8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQnOhsto7kcf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67p3sXLm7kaB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odJhv0Gn7kXJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiXwElOa7kUm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqsj1dcR7kSK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVp8fbSC7kPs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8igpOmPl7kNS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiA-XoudBTrB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55mmsmBEAO9V"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPCDNfwn+ZSjwUdIBC879Gv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}